{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ridge regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Gt48gOZHVVD0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Ridge regression main files\n",
        "Code editor: Xinyi Li, Yinchuan Li. Date: 2019.2.20.\n",
        "\n",
        "The code is run on Google's Colaboratory with Python 3.\n",
        "\n",
        "Paper: Mid-LSTM meets Mid-ARMA: deep learning for midterm stock prediction."
      ]
    },
    {
      "metadata": {
        "id": "N5n2u8FofNB3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Import packages"
      ]
    },
    {
      "metadata": {
        "id": "MxtdR87zk4ee",
        "colab_type": "code",
        "outputId": "433b8cf4-b863-4a39-8e95-84ba087e89fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "\n",
        "import os\n",
        "os.chdir(\"drive/download data/sp500new\")\n",
        "# !ls\n",
        "from google.colab import files\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "from numpy import newaxis\n",
        "from keras.layers import Dense, Activation, Dropout, LSTM\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "from math import pi,sqrt,exp,pow,log\n",
        "from numpy.linalg import det, inv\n",
        "from abc import ABCMeta, abstractmethod\n",
        "from sklearn import cluster\n",
        "\n",
        "import statsmodels.api as sm \n",
        "import scipy.stats as scs\n",
        "import scipy.optimize as sco\n",
        "import scipy.interpolate as sci\n",
        "from scipy import stats"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 131322 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.1-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
            "  from pandas.core import datetools\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "LUi18rtrlMir",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def stock_ridge_loop (filename):\n",
        "\n",
        "  split = (0.85);\n",
        "  sequence_length=60;\n",
        "  normalise= True\n",
        "  batch_size=60;\n",
        "  input_dim=4\n",
        "  input_timesteps=59\n",
        "  neurons=60\n",
        "  epochs=2\n",
        "  prediction_len=60\n",
        "  dense_output=4\n",
        "  window_size=60\n",
        "\n",
        "  dataframe = pd.read_csv(filename)\n",
        "  \n",
        "  #pre stock put on first\n",
        "  cols = ['Close_y', 'Volume_y','Close_x'];#corr will be add on later\n",
        "\n",
        "\n",
        "  len_dataframe=dataframe.shape[0]\n",
        "  corr_num=int(len_dataframe/sequence_length)\n",
        "  remainder=len_dataframe-corr_num*sequence_length\n",
        "\n",
        "  # caculate corr table\n",
        "  corr_win=[]\n",
        "  corr=np.zeros((len_dataframe))\n",
        "  for i in range(0,corr_num):\n",
        "    stock1=[]\n",
        "    stock2=[]\n",
        "    for j in range(i*sequence_length,i*sequence_length+sequence_length):\n",
        "      stock1.append(dataframe[cols[0]][j])\n",
        "      stock2.append(dataframe[cols[2]][j])\n",
        "    corr_win.append(np.corrcoef(stock1, stock2)[0,1])\n",
        "    for j in range(i*sequence_length,i*sequence_length+sequence_length):\n",
        "      corr[j]=corr_win[i]\n",
        "\n",
        "\n",
        "  corr_win_remainder=[] \n",
        "  stock1_remainder=[]\n",
        "  stock2_remainder=[] \n",
        "  for k in range(0,remainder):\n",
        "    stock1_remainder.append(dataframe[cols[0]][corr_num*sequence_length+k])\n",
        "    stock2_remainder.append(dataframe[cols[2]][corr_num*sequence_length+k])\n",
        "  corr_win_remainder.append(np.corrcoef(stock1_remainder, stock2_remainder)[0,1])\n",
        "  for q in range(0,remainder):\n",
        "    corr[corr_num*sequence_length+q]=corr_win_remainder[0]\n",
        "\n",
        "  i_split = int(len(dataframe) * split)\n",
        "  data_train = dataframe.get(cols).values[:i_split]\n",
        "  data_test  = dataframe.get(cols).values[i_split:]\n",
        "  len_train  = len(data_train)\n",
        "  len_test   = len(data_test)\n",
        "  len_train_windows = None\n",
        "\n",
        "  corr_df=pd.DataFrame(corr)\n",
        "  data_corr_train=corr_df.values[:i_split]\n",
        "  data_corr_test=corr_df.values[i_split:]\n",
        "\n",
        "  #get_test_data  ##############################################################\n",
        "  data_windows = []\n",
        "  for i in range(len_test - sequence_length):\n",
        "    data_windows.append(data_test[i:i+sequence_length])\n",
        "  data_windows = np.array(data_windows).astype(float)\n",
        "\n",
        "  # get original y_test\n",
        "  y_test_ori = data_windows[:, -1, [0]]\n",
        "\n",
        "  window_data=data_windows\n",
        "  win_num=window_data.shape[0]\n",
        "  row_num=window_data.shape[1]\n",
        "  col_num=window_data.shape[2]\n",
        "  normalised_data = []\n",
        "  record_min=[]\n",
        "  record_max=[]\n",
        "\n",
        "  for win_i in range(0,win_num):\n",
        "    normalised_window = []\n",
        "    for col_i in range(0,col_num):\n",
        "      temp_col=window_data[win_i,:,col_i]\n",
        "      temp_min=min(temp_col)\n",
        "      if col_i==0:\n",
        "        record_min.append(temp_min)#record min\n",
        "      temp_col=temp_col-temp_min\n",
        "      temp_max=max(temp_col)\n",
        "      if col_i==0:\n",
        "        record_max.append(temp_max)#record max\n",
        "      temp_col=temp_col/temp_max\n",
        "      normalised_window.append(temp_col)\n",
        "    normalised_window = np.array(normalised_window).T\n",
        "    normalised_data.append(normalised_window)\n",
        "  normalised_data=np.array(normalised_data)\n",
        "\n",
        "  corr_windows = []\n",
        "  for i in range(len_test - sequence_length):\n",
        "    corr_windows.append(data_corr_test[i:i+sequence_length])\n",
        "  corr_windows = np.array(corr_windows).astype(float)\n",
        "\n",
        "  get_test_data=[]\n",
        "  for win_i in range(0,win_num):\n",
        "    df1=pd.DataFrame(normalised_data[win_i,:,:])\n",
        "    df1['corr']=corr_windows[win_i,:,:]\n",
        "    df2=df1.values\n",
        "    get_test_data.append(df2)\n",
        "  get_test_data=np.array(get_test_data)  \n",
        " \n",
        "  data_windows=get_test_data\n",
        "  x_test = data_windows[:, :-1]\n",
        "  y_test = data_windows[:, -1, [0]]\n",
        "\n",
        "  #get_train_data #############################################################\n",
        "  data_windows = []\n",
        "  for i in range(len_train - sequence_length):\n",
        "    data_windows.append(data_train[i:i+sequence_length])\n",
        "  data_windows = np.array(data_windows).astype(float)\n",
        "\n",
        "  window_data=data_windows\n",
        "  win_num=window_data.shape[0]\n",
        "  row_num=window_data.shape[1]\n",
        "  col_num=window_data.shape[2]\n",
        "\n",
        "  normalised_data = []\n",
        "  for win_i in range(0,win_num):\n",
        "    normalised_window = []\n",
        "    for col_i in range(0,col_num):\n",
        "      temp_col=window_data[win_i,:,col_i]\n",
        "      temp_min=min(temp_col)\n",
        "      temp_col=temp_col-temp_min\n",
        "      temp_max=max(temp_col)\n",
        "      temp_col=temp_col/temp_max\n",
        "      normalised_window.append(temp_col)\n",
        "    normalised_window = np.array(normalised_window).T\n",
        "    normalised_data.append(normalised_window)\n",
        "  normalised_data=np.array(normalised_data)\n",
        "\n",
        "  corr_windows_train = []\n",
        "  for i in range(len_train - sequence_length):\n",
        "    corr_windows_train.append(data_corr_train[i:i+sequence_length])\n",
        "  corr_windows_train = np.array(corr_windows_train).astype(float)\n",
        "\n",
        "  get_train_data=[]\n",
        "  for win_i in range(0,win_num):\n",
        "    df1=pd.DataFrame(normalised_data[win_i,:,:])\n",
        "    df1['corr']=corr_windows_train[win_i,:,:]\n",
        "    df2=df1.values\n",
        "    get_train_data.append(df2)\n",
        "  get_train_data=np.array(get_train_data)  \n",
        "\n",
        "  data_windows=get_train_data\n",
        "  x_train = data_windows[:, :-1]\n",
        "  y_train = data_windows[:, -1]\n",
        "\n",
        "  ##3 ridge regression\n",
        "  x_train_ri=pd.DataFrame(x_train[:,:,0])\n",
        "  y_train_ri=pd.DataFrame(y_train[:,0])\n",
        "  x_test_ri=pd.DataFrame(x_test[:,:,0])\n",
        "  y_test_ri=pd.DataFrame(y_test[:,0])\n",
        "  y_test_ori_ri=pd.DataFrame(y_test_ori[:,0])\n",
        "\n",
        "  clf = Ridge(alpha=1.0)\n",
        "  clf.fit(x_train_ri, y_train_ri) \n",
        "\n",
        "  data=x_test[:,:,0]\n",
        "  \n",
        "  prediction_seqs = []\n",
        "  pre_win_num=int(len(data)/prediction_len)\n",
        "  window_size=sequence_length\n",
        "\n",
        "  for i in range(0,pre_win_num):\n",
        "    curr_frame = data[i*prediction_len]\n",
        "    predicted = []\n",
        "\n",
        "    for j in range(0,prediction_len):\n",
        "      predicted.append(clf.predict(curr_frame[newaxis,:])[0])\n",
        "      curr_frame = curr_frame[1:]\n",
        "      curr_frame = np.insert(curr_frame, [window_size-2], predicted[-1], axis=0)\n",
        "    prediction_seqs.append(predicted)\n",
        "\n",
        "  #de_predicted\n",
        "  de_predicted=[]\n",
        "  len_pre_win=int(len(data)/prediction_len)\n",
        "  len_pre=prediction_len\n",
        "  \n",
        "  m=0\n",
        "  for i in range(0,len_pre_win):\n",
        "    for j in range(0,len_pre):\n",
        "      de_predicted.append(prediction_seqs[i][j]*record_max[m]+record_min[m])\n",
        "      m=m+1\n",
        "\n",
        "  error = []\n",
        "  diff=y_test.shape[0]-prediction_len*pre_win_num\n",
        "\n",
        "  for i in range(y_test_ori.shape[0]-diff):\n",
        "      error.append(y_test_ori[i,] - de_predicted[i])\n",
        "\n",
        "  squaredError = []\n",
        "  absError = []\n",
        "  for val in error:\n",
        "      squaredError.append(val * val) \n",
        "      absError.append(abs(val))\n",
        "\n",
        "  error_percent=[]\n",
        "  for i in range(len(error)):\n",
        "    val=absError[i]/y_test_ori[i,]\n",
        "    val=abs(val)\n",
        "    error_percent.append(val)\n",
        "\n",
        "  mean_error_percent=sum(error_percent) / len(error_percent)\n",
        "  accuracy=1-mean_error_percent\n",
        "\n",
        "  MSE=sum(squaredError) / len(squaredError)\n",
        "  return MSE,accuracy,y_test_ori,de_predicted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6TVhkdD1lZFg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filename=np.load('filename_delete_sort.npy')\n",
        "\n",
        "result_ridge_df=pd.DataFrame(columns=('index','stock','MSE','accuracy','true','predict'))\n",
        "n=len(filename)\n",
        "                           \n",
        "for i in range(0,n):\n",
        "  index=i\n",
        "  stock=filename[i]\n",
        "  result=stock_ridge_loop(filename[i])\n",
        "  MSE=result[0]\n",
        "  accuracy=result[1]\n",
        "  true=result[2]\n",
        "  predict=result[3]\n",
        "  result_ridge_df=result_ridge_df.append(pd.DataFrame({'index':[index],\n",
        "                                                     'stock':[stock],\n",
        "                                                     'MSE':[MSE],\n",
        "                                                     'accuracy':[accuracy],\n",
        "                                                     'true':[true],\n",
        "                                                     'predict':[predict]}),ignore_index=True)\n",
        "  print(i)\n",
        "  np.save('ridge_d0.npy',result_ridge_df)\n",
        "  result_ridge_df.to_csv('ridge_d0.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zo4ljZsQlg4e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Reshape data\n",
        "L4_all=np.load('ridge_d0.npy')\n",
        "L4_all=pd.DataFrame(L4_all)\n",
        "L4_all.columns=['MSE','accuracy','index','predict','stock','TRUE']\n",
        "\n",
        "#accuracy only\n",
        "filename=np.load('filename_delete_sort.npy')\n",
        "n=len(filename)\n",
        "len_pre=360\n",
        "\n",
        "result_df=pd.DataFrame(columns=('index','stock','TRUE','predict','accuracy','MSE'))\n",
        "for i in range(0,n):\n",
        "  index=i\n",
        "  stock=filename[i]\n",
        "  #TRUE\n",
        "  t=[]\n",
        "  for j in range(0,len_pre):\n",
        "    t.append(L4_all['TRUE'][i][j][0])\n",
        "  TRUE=t\n",
        "  #predict\n",
        "  t=[]\n",
        "  for j in range(0,len_pre):\n",
        "    t.append(L4_all['predict'][i][j][0])\n",
        "  predict=t\n",
        "  #accuracy\n",
        "  accuracy=[]\n",
        "  for j in range(0,len_pre):\n",
        "    t=abs(TRUE[j]-predict[j])/TRUE[j]\n",
        "    t1=1-t\n",
        "    accuracy.append(t1)\n",
        "  accuracy=accuracy\n",
        "  #MSE\n",
        "  MSE=L4_all['MSE'][i][0]\n",
        "  result_df=result_df.append(pd.DataFrame({'index':index,'stock':[stock],\n",
        "                                           'TRUE':[TRUE],\n",
        "                                           'predict':[predict],\n",
        "                                          'accuracy':[accuracy],\n",
        "                                          'MSE':[MSE]}),\n",
        "                             ignore_index=True)\n",
        "  print(i)\n",
        "np.save('ridge_r_new.npy',result_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "loEvF9Vzl5q6",
        "colab_type": "code",
        "outputId": "c311d256-c38f-44b0-a664-cead4de8032a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "##Mean MPA of all stocks\n",
        "ridge_r=np.load('ridge_r_new.npy')\n",
        "ridge_r=pd.DataFrame(ridge_r)\n",
        "ridge_r.columns=['MSE','TRUE','accuracy','index','predict','stock']\n",
        "\n",
        "n=451\n",
        "avg_accuracy1=[]\n",
        "for i in range(0,360):\n",
        "  t1=0\n",
        "  for j in range(0,n):\n",
        "    t1=t1+ridge_r['accuracy'][j][i]\n",
        "  t1=t1/n\n",
        "  avg_accuracy1.append(t1)\n",
        "\n",
        "half1=[]\n",
        "for i in range(0,6): \n",
        "  half1.extend(avg_accuracy1[60*(i+1)-30:60*(i+1)]) \n",
        "mean1=pd.DataFrame(half1).mean()[0]\n",
        "\n",
        "print('Ridge regression Mean MPA:',mean1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ridge regression Mean MPA: 0.9252519275147825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dbYs00nL-6CK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Portfolio allocation"
      ]
    },
    {
      "metadata": {
        "id": "-0sSA6vE-8kT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Mean variance portfolio allocation based on ridge regression (0-60 days) , asset 1."
      ]
    },
    {
      "metadata": {
        "id": "26XQMWal_FAJ",
        "colab_type": "code",
        "outputId": "f42fe190-26d7-4ba2-984f-6084b4fb4326",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "cell_type": "code",
      "source": [
        "LH_dph=np.load('ridge_r_new.npy')\n",
        "LH_dph=pd.DataFrame(LH_dph)\n",
        "LH_dph.columns = ['MSE','TRUE','accuracy','index','predict','stock']\n",
        "\n",
        "test_win=6\n",
        "pre_len1=60\n",
        "pre_len2=60\n",
        "stock_len=451\n",
        "rf=0.015#risk free rate\n",
        "\n",
        "df_all=pd.DataFrame(columns=('index','return_pre','variance_pre','sharp_pre',\n",
        "                            'return_true','variance_true','sharp_true',))\n",
        "\n",
        "filename=np.load('filename_delete_sort.npy')\n",
        "df = pd.DataFrame()\n",
        "\n",
        "####cumulative\n",
        "for k in range(0,test_win):\n",
        "  n=stock_len\n",
        "  \n",
        "  for i in range(0,n):\n",
        "    t=LH_dph['predict'][i][k*pre_len1:k*pre_len1+pre_len1]\n",
        "    t1=[]\n",
        "    for j in range(0,pre_len1):\n",
        "      t1.append(t[j])\n",
        "    t=t1\n",
        "    df[filename[i]]=t  \n",
        "  data1=df\n",
        "  log_returns = np.log(data1 / data1.shift(1))\n",
        "  ret_index = (1 + log_returns).cumprod()\n",
        "  \n",
        "  choose_name=[]\n",
        "  choose_index=[]\n",
        "  for i in range(0,n):\n",
        "    if ret_index[filename[i]][59]>1.15:\n",
        "      choose_name.append(filename[i])\n",
        "      choose_index.append(i)\n",
        "      \n",
        "  #choose data\n",
        "  m=len(choose_index)\n",
        "  data2=data1.iloc[:,choose_index]\n",
        "  log_returns = np.log(data2 / data2.shift(1))\n",
        "  \n",
        "  rets = log_returns\n",
        "  year_ret = rets.mean() * 252\n",
        "  year_volatility = rets.cov() * 252\n",
        "  number_of_assets = m\n",
        "  weights = np.random.random(number_of_assets)\n",
        "  weights /= np.sum(weights)\n",
        "  \n",
        "  def statistics(weights):        \n",
        "    weights = np.array(weights)\n",
        "    pret = np.sum(rets.mean() * weights) * 252\n",
        "    pvol = np.sqrt(np.dot(weights.T, np.dot(rets.cov() * 252, weights)))\n",
        "    return np.array([pret, pvol, (pret-rf) / pvol])\n",
        "  \n",
        "  def min_func_sharpe(weights):\n",
        "    return -statistics(weights)[2]\n",
        "    \n",
        "  bnds = tuple((0,1) for x in range(number_of_assets))\n",
        "  cons = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
        "\n",
        "  opts = sco.minimize(min_func_sharpe, \n",
        "                      number_of_assets * [1. / number_of_assets,],\n",
        "                      method='SLSQP', \n",
        "                      bounds=bnds, \n",
        "                      constraints=cons)\n",
        "  \n",
        "  weights_pre=opts['x']\n",
        "  \n",
        "  ##check return\n",
        "  df2 = pd.DataFrame()\n",
        "  for i in choose_index:\n",
        "    t=LH_dph['TRUE'][i][k*pre_len1:k*pre_len1+pre_len1]\n",
        "    t1=[]\n",
        "    for j in range(0,pre_len1):\n",
        "      t1.append(t[j])\n",
        "    t= t1\n",
        "    df2['ture'+filename[i]]=t\n",
        "  data3=df2 \n",
        "  log_returns_true = np.log(data3 / data3.shift(1))\n",
        "  \n",
        "  rets_true = log_returns_true\n",
        "  year_ret_true = rets_true.mean() * 252\n",
        "  year_volatility_true = rets_true.cov() * 252\n",
        "  number_of_assets = m  #real asset number\n",
        "  \n",
        "  def statistics_true(weights):        \n",
        "    weights = np.array(weights)\n",
        "    pret = np.sum(rets_true.mean() * weights) * 252\n",
        "    pvol = np.sqrt(np.dot(weights.T, np.dot(rets_true.cov() * 252, weights)))\n",
        "    return np.array([pret, pvol, (pret-rf) / pvol])\n",
        "  \n",
        "  index=k\n",
        "  return_pre=statistics(opts['x'])[0]\n",
        "  variance_pre=statistics(opts['x'])[1]\n",
        "  sharp_pre=statistics(opts['x'])[2]\n",
        "  return_true=statistics_true(opts['x'])[0]\n",
        "  variance_true=statistics_true(opts['x'])[1]\n",
        "  sharp_true=statistics_true(opts['x'])[2]\n",
        "  \n",
        "  df_all=df_all.append(pd.DataFrame({'index':[index],\n",
        "                                     'return_pre':[return_pre],\n",
        "                                     'variance_pre':[variance_pre],\n",
        "                                     'sharp_pre':[sharp_pre],\n",
        "                                     'return_true':[ return_true],\n",
        "                                     'variance_true':[variance_true],\n",
        "                                    'sharp_true':[sharp_true],}),ignore_index=True)\n",
        "#   print('stock number:',n)\n",
        "#   print('count',k)\n",
        "#   print('choose number',m)\n",
        "#   print('initial random weight',weights)\n",
        "#   print('pre weights',opts['x'])\n",
        "print('Mean variance portfolio allocation based on ridge regression (0-60 days) , all stocks: \\n',df_all)\n",
        "#   print('weight',opts['x'].round(3))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean variance portfolio allocation based on ridge regression (0-60 days) , all stocks: \n",
            "   index  return_pre  return_true  sharp_pre  sharp_true  variance_pre  \\\n",
            "0     0    0.796437     0.783196  16.881991    4.670287      0.046288   \n",
            "1     1    0.918241     1.222854  26.208353   10.367646      0.034464   \n",
            "2     2    0.744868     0.568767   8.884582    2.144261      0.082150   \n",
            "3     3    0.798062     0.692123  43.344174    4.628260      0.018066   \n",
            "4     4    0.761791     0.807338  43.119256    7.820320      0.017319   \n",
            "5     5    0.785273     0.755630  12.762811    3.337458      0.060353   \n",
            "\n",
            "   variance_true  \n",
            "0       0.164486  \n",
            "1       0.116502  \n",
            "2       0.258255  \n",
            "3       0.146302  \n",
            "4       0.101318  \n",
            "5       0.221914  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZkllcLuV_rKV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Minimum variance portfolio allocation based on ridge regression (0-60 days) , asset 1."
      ]
    },
    {
      "metadata": {
        "id": "AxX8SuFq_0FD",
        "colab_type": "code",
        "outputId": "134f18ab-1f1f-4ea6-ee81-6f1e1e9ab0b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "cell_type": "code",
      "source": [
        "LH_dph=np.load('ridge_r_new.npy')\n",
        "LH_dph=pd.DataFrame(LH_dph)\n",
        "LH_dph.columns = ['MSE','TRUE','accuracy','index','predict','stock']\n",
        "\n",
        "test_win=6\n",
        "pre_len1=60\n",
        "pre_len2=60\n",
        "stock_len=451\n",
        "rf=0.015#risk free rate\n",
        "\n",
        "df_all=pd.DataFrame(columns=('index','return_pre','variance_pre','sharp_pre',\n",
        "                            'return_true','variance_true','sharp_true',))\n",
        "\n",
        "filename=np.load('filename_delete_sort.npy')\n",
        "df = pd.DataFrame()\n",
        "\n",
        "####cumulative\n",
        "for k in range(0,test_win):\n",
        "  n=stock_len\n",
        "  \n",
        "  for i in range(0,n):\n",
        "    t=LH_dph['predict'][i][k*pre_len1:k*pre_len1+pre_len1]\n",
        "    t1=[]\n",
        "    for j in range(0,pre_len1):\n",
        "      t1.append(t[j])\n",
        "    t=t1\n",
        "    df[filename[i]]=t  \n",
        "  data1=df\n",
        "  log_returns = np.log(data1 / data1.shift(1))\n",
        "  ret_index = (1 + log_returns).cumprod()\n",
        "  \n",
        "  choose_name=[]\n",
        "  choose_index=[]\n",
        "  for i in range(0,n):\n",
        "    if ret_index[filename[i]][59]>1.15:\n",
        "      choose_name.append(filename[i])\n",
        "      choose_index.append(i)\n",
        "      \n",
        "  #choose data\n",
        "  m=len(choose_index)\n",
        "  data2=data1.iloc[:,choose_index]\n",
        "  log_returns = np.log(data2 / data2.shift(1))\n",
        "  \n",
        "  rets = log_returns\n",
        "  year_ret = rets.mean() * 252\n",
        "  year_volatility = rets.cov() * 252\n",
        "  number_of_assets = m\n",
        "  weights = np.random.random(number_of_assets)\n",
        "  weights /= np.sum(weights)\n",
        "  \n",
        "  def statistics(weights):        \n",
        "    weights = np.array(weights)\n",
        "    pret = np.sum(rets.mean() * weights) * 252\n",
        "    pvol = np.sqrt(np.dot(weights.T, np.dot(rets.cov() * 252, weights)))\n",
        "    return np.array([pret, pvol, (pret-rf) / pvol])\n",
        "  \n",
        "  def min_func_sharpe(weights):\n",
        "    return statistics(weights)[1]\n",
        "    \n",
        "  bnds = tuple((0,1) for x in range(number_of_assets))\n",
        "  cons = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
        "\n",
        "  opts = sco.minimize(min_func_sharpe, \n",
        "                      number_of_assets * [1. / number_of_assets,],\n",
        "                      method='SLSQP', \n",
        "                      bounds=bnds, \n",
        "                      constraints=cons)\n",
        "  \n",
        "  weights_pre=opts['x']\n",
        "  \n",
        "  ##check return\n",
        "  df2 = pd.DataFrame()\n",
        "  for i in choose_index:\n",
        "    t=LH_dph['TRUE'][i][k*pre_len1:k*pre_len1+pre_len1]\n",
        "    t1=[]\n",
        "    for j in range(0,pre_len1):\n",
        "      t1.append(t[j])\n",
        "    t= t1\n",
        "    df2['ture'+filename[i]]=t\n",
        "  data3=df2 \n",
        "  log_returns_true = np.log(data3 / data3.shift(1))\n",
        "  \n",
        "  rets_true = log_returns_true\n",
        "  year_ret_true = rets_true.mean() * 252\n",
        "  year_volatility_true = rets_true.cov() * 252\n",
        "  number_of_assets = m  #real asset number\n",
        "  \n",
        "  def statistics_true(weights):        \n",
        "    weights = np.array(weights)\n",
        "    pret = np.sum(rets_true.mean() * weights) * 252\n",
        "    pvol = np.sqrt(np.dot(weights.T, np.dot(rets_true.cov() * 252, weights)))\n",
        "    return np.array([pret, pvol, (pret-rf) / pvol])\n",
        "  \n",
        "  index=k\n",
        "  return_pre=statistics(opts['x'])[0]\n",
        "  variance_pre=statistics(opts['x'])[1]\n",
        "  sharp_pre=statistics(opts['x'])[2]\n",
        "  return_true=statistics_true(opts['x'])[0]\n",
        "  variance_true=statistics_true(opts['x'])[1]\n",
        "  sharp_true=statistics_true(opts['x'])[2]\n",
        "  \n",
        "  df_all=df_all.append(pd.DataFrame({'index':[index],\n",
        "                                     'return_pre':[return_pre],\n",
        "                                     'variance_pre':[variance_pre],\n",
        "                                     'sharp_pre':[sharp_pre],\n",
        "                                     'return_true':[ return_true],\n",
        "                                     'variance_true':[variance_true],\n",
        "                                    'sharp_true':[sharp_true],}),ignore_index=True)\n",
        "#   print('stock number:',n)\n",
        "#   print('count',k)\n",
        "#   print('choose number',m)\n",
        "#   print('initial random weight',weights)\n",
        "#   print('pre weights',opts['x'])\n",
        "print('Minimum variance portfolio allocation based on ridge regression (0-60 days) , all stocks: \\n',df_all)\n",
        "#   print('weight',opts['x'].round(3))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Minimum variance portfolio allocation based on ridge regression (0-60 days) , all stocks: \n",
            "   index  return_pre  return_true  sharp_pre  sharp_true  variance_pre  \\\n",
            "0     0    0.753332     0.733688  16.425097    4.322717      0.044951   \n",
            "1     1    0.872483     1.158078  25.569300    9.588978      0.033536   \n",
            "2     2    0.708565     0.605436   8.662348    2.343849      0.080067   \n",
            "3     3    0.716640     0.590743  41.110297    4.185277      0.017067   \n",
            "4     4    0.718246     0.768684  41.825964    7.685020      0.016814   \n",
            "5     5    0.756526     0.676397  12.522136    3.100964      0.059217   \n",
            "\n",
            "   variance_true  \n",
            "0       0.166258  \n",
            "1       0.119208  \n",
            "2       0.251909  \n",
            "3       0.137564  \n",
            "4       0.098072  \n",
            "5       0.213288  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fXVk9BGUAZVJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Mean variance portfolio allocation based on ridge regression (0-60 days) , asset 2."
      ]
    },
    {
      "metadata": {
        "id": "xTXMwQ_dAk7w",
        "colab_type": "code",
        "outputId": "29a4ce1a-a2c0-4564-d341-4e6f515135b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "cell_type": "code",
      "source": [
        "filename=np.load('filename_delete_sort.npy')\n",
        "sort=np.load('new_sortname.npy')\n",
        "\n",
        "LH_dph=np.load('ridge_r_new.npy')\n",
        "LH_dph=pd.DataFrame(LH_dph)\n",
        "LH_dph.columns = ['MSE','TRUE','accuracy','index','predict','stock']\n",
        "LH_dph_ori=LH_dph\n",
        "\n",
        "top=50\n",
        "\n",
        "LH_dph=[]\n",
        "for i in range(0,451):\n",
        "  for j in range(0,top):\n",
        "    if LH_dph_ori['stock'][i]==sort[j]:\n",
        "      LH_dph.append(LH_dph_ori.iloc[i,])\n",
        "      \n",
        "LH_dph=np.array(LH_dph)\n",
        "LH_dph=pd.DataFrame(LH_dph)\n",
        "LH_dph.columns = ['MSE','TRUE','accuracy','index','predict','stock']\n",
        "\n",
        "test_win=6\n",
        "pre_len1=60\n",
        "pre_len2=60\n",
        "rf=0.015\n",
        "stock_len=top\n",
        "\n",
        "df_all=pd.DataFrame(columns=('index','return_pre','variance_pre','sharp_pre',\n",
        "                            'return_true','variance_true','sharp_true',))\n",
        "\n",
        "filename=np.load('filename_delete_sort.npy')\n",
        "df = pd.DataFrame()\n",
        "\n",
        "rets_true_all=pd.DataFrame()\n",
        "####cumulative\n",
        "for k in range(0,test_win):\n",
        "  n=stock_len\n",
        "  \n",
        "  for i in range(0,n):\n",
        "    t=LH_dph['predict'][i][k*pre_len1:k*pre_len1+pre_len1]\n",
        "    t1=[]\n",
        "    for j in range(0,pre_len1):\n",
        "      t1.append(t[j])\n",
        "    t=t1\n",
        "    df[filename[i]]=t  \n",
        "  data1=df\n",
        "  log_returns = np.log(data1 / data1.shift(1))\n",
        "  ret_index = (1 + log_returns).cumprod()\n",
        "  \n",
        "  choose_name=[]\n",
        "  choose_index=[]\n",
        "  for i in range(0,n):\n",
        "    if ret_index[filename[i]][59]>1.03:\n",
        "        choose_name.append(filename[i])\n",
        "        choose_index.append(i)\n",
        "\n",
        "  #choose data\n",
        "  m=len(choose_index)\n",
        "  data2=data1.iloc[:,choose_index]\n",
        "  log_returns = np.log(data2 / data2.shift(1))\n",
        "  \n",
        "  rets = log_returns\n",
        "  number_of_assets = m\n",
        "  weights = np.random.random(number_of_assets)\n",
        "  weights /= np.sum(weights)\n",
        "  \n",
        "  def statistics(weights):        \n",
        "    weights = np.array(weights)\n",
        "    pret = np.sum(rets.mean() * weights) * 252\n",
        "    pvol = np.sqrt(np.dot(weights.T, np.dot(rets.cov() * 252, weights)))\n",
        "    return np.array([pret, pvol, (pret-rf) / pvol])\n",
        "  \n",
        "  def min_func_sharpe(weights):\n",
        "    return -statistics(weights)[2]\n",
        "  \n",
        "  bnds = tuple((0,1) for x in range(number_of_assets))\n",
        "  cons = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
        "\n",
        "  opts = sco.minimize(min_func_sharpe, \n",
        "                      number_of_assets * [1. / number_of_assets,],\n",
        "                      method='SLSQP', \n",
        "                      bounds=bnds, \n",
        "                      constraints=cons)\n",
        "  \n",
        "  weights_pre=opts['x']\n",
        "  \n",
        "  ##check return\n",
        "  df2 = pd.DataFrame()\n",
        "  for i in choose_index:\n",
        "    t=LH_dph['TRUE'][i][k*pre_len1:k*pre_len1+pre_len1]\n",
        "    t1=[]\n",
        "    for j in range(0,pre_len1):\n",
        "      t1.append(t[j])\n",
        "    t= t1\n",
        "    df2['ture'+filename[i]]=t\n",
        "  data3=df2 \n",
        "  log_returns_true = np.log(data3 / data3.shift(1))\n",
        "  \n",
        "  rets_true = log_returns_true\n",
        "  a=rets_true_all.append(rets_true,ignore_index=True) \n",
        "  number_of_assets = m  #real asset number\n",
        "  \n",
        "  def statistics_true(weights):        \n",
        "    weights = np.array(weights)\n",
        "    pret = np.sum(rets_true.mean() * weights) * 252\n",
        "    pvol = np.sqrt(np.dot(weights.T, np.dot(rets_true.cov() * 252, weights)))\n",
        "    return np.array([pret, pvol, (pret-rf) / pvol])\n",
        "  \n",
        "  index=k\n",
        "  return_pre=statistics(opts['x'])[0]\n",
        "  variance_pre=statistics(opts['x'])[1]\n",
        "  sharp_pre=statistics(opts['x'])[2]\n",
        "  return_true=statistics_true(opts['x'])[0]\n",
        "  variance_true=statistics_true(opts['x'])[1]\n",
        "  sharp_true=statistics_true(opts['x'])[2]\n",
        "  \n",
        "  df_all=df_all.append(pd.DataFrame({'index':[index],\n",
        "                                     'return_pre':[return_pre],\n",
        "                                     'variance_pre':[variance_pre],\n",
        "                                     'sharp_pre':[sharp_pre],\n",
        "                                     'return_true':[ return_true],\n",
        "                                     'variance_true':[variance_true],\n",
        "                                    'sharp_true':[sharp_true],}),ignore_index=True)\n",
        "#   print('stock number:',n)\n",
        "#   print('count',k)\n",
        "#   print('choose number',m)\n",
        "#   print('initial random weight',weights)\n",
        "#   print('pre weights',opts['x'])\n",
        "#   print('weight',opts['x'].round(3))\n",
        "print('Mean variance portfolio allocation based on ridge regression (0-60 days) , asset 2: \\n',df_all)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean variance portfolio allocation based on ridge regression (0-60 days) , asset 2: \n",
            "   index  return_pre  return_true  sharp_pre  sharp_true  variance_pre  \\\n",
            "0     0    0.220958     0.251311  18.170321    2.685436      0.011335   \n",
            "1     1    0.228817     0.210891  12.966416    2.819985      0.016490   \n",
            "2     2    0.355898     0.247504  10.967833    1.291678      0.031082   \n",
            "3     3    0.437225     0.256502  25.027221    1.995265      0.016871   \n",
            "4     4    0.375218     0.331941  34.846071    3.424521      0.010337   \n",
            "5     5    0.267181     0.142459  15.124748    0.874061      0.016673   \n",
            "\n",
            "   variance_true  \n",
            "0       0.087997  \n",
            "1       0.069465  \n",
            "2       0.180001  \n",
            "3       0.121037  \n",
            "4       0.092551  \n",
            "5       0.145824  \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}