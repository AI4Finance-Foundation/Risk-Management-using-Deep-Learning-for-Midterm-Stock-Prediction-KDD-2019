{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "random forest.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "OvxUIX93Trfy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Random forest main files\n",
        "Code editor: Xinyi Li, Yinchuan Li. Date: 2019.2.20.\n",
        "\n",
        "The code is run on Google's Colaboratory with Python 3.\n",
        "\n",
        "Paper: Mid-LSTM meets Mid-ARMA: deep learning for midterm stock prediction."
      ]
    },
    {
      "metadata": {
        "id": "zJOXgy5XmmnG",
        "colab_type": "code",
        "outputId": "477ed4ce-23f2-4d8c-e22a-328b33b8ee58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "\n",
        "import os\n",
        "os.chdir(\"drive/download data/sp500new\")\n",
        "# !ls\n",
        "from google.colab import files\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "from numpy import newaxis\n",
        "from keras.layers import Dense, Activation, Dropout, LSTM\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "from math import pi,sqrt,exp,pow,log\n",
        "from numpy.linalg import det, inv\n",
        "from abc import ABCMeta, abstractmethod\n",
        "from sklearn import cluster\n",
        "\n",
        "import statsmodels.api as sm \n",
        "import scipy.stats as scs\n",
        "import scipy.optimize as sco\n",
        "import scipy.interpolate as sci\n",
        "from scipy import stats"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 131322 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.1-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
            "  from pandas.core import datetools\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "3miPsQNSnF2D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def stock_RF_loop (filename):\n",
        "\n",
        "  split = (0.85);\n",
        "  sequence_length=60;\n",
        "  normalise= True\n",
        "  batch_size=60;\n",
        "  input_dim=4\n",
        "  input_timesteps=sequence_length-1\n",
        "  neurons=60\n",
        "  epochs=2\n",
        "  prediction_len=60\n",
        "  dense_output=4\n",
        "  window_size=sequence_length\n",
        "\n",
        "  dataframe = pd.read_csv(filename)\n",
        "  \n",
        "  #pre stock put on first\n",
        "  cols = ['Close_y', 'Volume_y','Close_x'];#corr will be add on later\n",
        "\n",
        "\n",
        "  len_dataframe=dataframe.shape[0]\n",
        "  corr_num=int(len_dataframe/sequence_length)\n",
        "  remainder=len_dataframe-corr_num*sequence_length\n",
        "\n",
        "  # caculate corr table\n",
        "  corr_win=[]\n",
        "  corr=np.zeros((len_dataframe))\n",
        "  for i in range(0,corr_num):\n",
        "    stock1=[]\n",
        "    stock2=[]\n",
        "    for j in range(i*sequence_length,i*sequence_length+sequence_length):\n",
        "      stock1.append(dataframe[cols[0]][j])\n",
        "      stock2.append(dataframe[cols[2]][j])\n",
        "    corr_win.append(np.corrcoef(stock1, stock2)[0,1])\n",
        "    for j in range(i*sequence_length,i*sequence_length+sequence_length):\n",
        "      corr[j]=corr_win[i]\n",
        "\n",
        "\n",
        "  corr_win_remainder=[] \n",
        "  stock1_remainder=[]\n",
        "  stock2_remainder=[] \n",
        "  for k in range(0,remainder):\n",
        "    stock1_remainder.append(dataframe[cols[0]][corr_num*sequence_length+k])\n",
        "    stock2_remainder.append(dataframe[cols[2]][corr_num*sequence_length+k])\n",
        "  corr_win_remainder.append(np.corrcoef(stock1_remainder, stock2_remainder)[0,1])\n",
        "  for q in range(0,remainder):\n",
        "    corr[corr_num*sequence_length+q]=corr_win_remainder[0]\n",
        "\n",
        "  i_split = int(len(dataframe) * split)\n",
        "  data_train = dataframe.get(cols).values[:i_split]\n",
        "  data_test  = dataframe.get(cols).values[i_split:]\n",
        "  len_train  = len(data_train)\n",
        "  len_test   = len(data_test)\n",
        "  len_train_windows = None\n",
        "\n",
        "  corr_df=pd.DataFrame(corr)\n",
        "  data_corr_train=corr_df.values[:i_split]\n",
        "  data_corr_test=corr_df.values[i_split:]\n",
        "\n",
        "  #get_test_data   #############################################################\n",
        "\n",
        "  data_windows = []\n",
        "  for i in range(len_test - sequence_length):\n",
        "    data_windows.append(data_test[i:i+sequence_length])\n",
        "  data_windows = np.array(data_windows).astype(float)\n",
        "\n",
        "  # get original y_test\n",
        "  y_test_ori = data_windows[:, -1, [0]]\n",
        "\n",
        "  window_data=data_windows\n",
        "  win_num=window_data.shape[0]\n",
        "  row_num=window_data.shape[1]\n",
        "  col_num=window_data.shape[2]\n",
        "  normalised_data = []\n",
        "  record_min=[]\n",
        "  record_max=[]\n",
        "\n",
        "  for win_i in range(0,win_num):\n",
        "    normalised_window = []\n",
        "    for col_i in range(0,col_num):\n",
        "      temp_col=window_data[win_i,:,col_i]\n",
        "      temp_min=min(temp_col)\n",
        "      if col_i==0:\n",
        "        record_min.append(temp_min)#record min\n",
        "      temp_col=temp_col-temp_min\n",
        "      temp_max=max(temp_col)\n",
        "      if col_i==0:\n",
        "        record_max.append(temp_max)#record max\n",
        "      temp_col=temp_col/temp_max\n",
        "      normalised_window.append(temp_col)\n",
        "    normalised_window = np.array(normalised_window).T\n",
        "    normalised_data.append(normalised_window)\n",
        "  normalised_data=np.array(normalised_data)\n",
        "\n",
        "  corr_windows = []\n",
        "  for i in range(len_test - sequence_length):\n",
        "    corr_windows.append(data_corr_test[i:i+sequence_length])\n",
        "  corr_windows = np.array(corr_windows).astype(float)\n",
        "\n",
        "  get_test_data=[]\n",
        "  for win_i in range(0,win_num):\n",
        "    df1=pd.DataFrame(normalised_data[win_i,:,:])\n",
        "    df1['corr']=corr_windows[win_i,:,:]\n",
        "    df2=df1.values\n",
        "    get_test_data.append(df2)\n",
        "  get_test_data=np.array(get_test_data)  \n",
        "\n",
        "  data_windows=get_test_data\n",
        "  x_test = data_windows[:, :-1]\n",
        "  y_test = data_windows[:, -1, [0]]\n",
        "\n",
        "  #get_train_data ##############################################################\n",
        "  data_windows = []\n",
        "  for i in range(len_train - sequence_length):\n",
        "    data_windows.append(data_train[i:i+sequence_length])\n",
        "  data_windows = np.array(data_windows).astype(float)\n",
        "\n",
        "  window_data=data_windows\n",
        "  win_num=window_data.shape[0]\n",
        "  row_num=window_data.shape[1]\n",
        "  col_num=window_data.shape[2]\n",
        "\n",
        "  normalised_data = []\n",
        "  for win_i in range(0,win_num):\n",
        "    normalised_window = []\n",
        "    for col_i in range(0,col_num):\n",
        "      temp_col=window_data[win_i,:,col_i]\n",
        "      temp_min=min(temp_col)\n",
        "      temp_col=temp_col-temp_min\n",
        "      temp_max=max(temp_col)\n",
        "      temp_col=temp_col/temp_max\n",
        "      normalised_window.append(temp_col)\n",
        "    normalised_window = np.array(normalised_window).T\n",
        "    normalised_data.append(normalised_window)\n",
        "  normalised_data=np.array(normalised_data)\n",
        "\n",
        "  corr_windows_train = []\n",
        "  for i in range(len_train - sequence_length):\n",
        "    corr_windows_train.append(data_corr_train[i:i+sequence_length])\n",
        "  corr_windows_train = np.array(corr_windows_train).astype(float)\n",
        "\n",
        "  get_train_data=[]\n",
        "  for win_i in range(0,win_num):\n",
        "    df1=pd.DataFrame(normalised_data[win_i,:,:])\n",
        "    df1['corr']=corr_windows_train[win_i,:,:]\n",
        "    df2=df1.values\n",
        "    get_train_data.append(df2)\n",
        "  get_train_data=np.array(get_train_data)  \n",
        "\n",
        "  data_windows=get_train_data\n",
        "  x_train = data_windows[:, :-1]\n",
        "  y_train = data_windows[:, -1]\n",
        "\n",
        "  ## Random forest\n",
        "  x_train_rf=pd.DataFrame(x_train[:,:,0])\n",
        "  y_train_rf=pd.DataFrame(y_train[:,0])\n",
        "  x_test_rf=pd.DataFrame(x_test[:,:,0])\n",
        "  y_test_rf=pd.DataFrame(y_test[:,0])\n",
        "  y_test_ori_rf=pd.DataFrame(y_test_ori[:,0])\n",
        "\n",
        "  forest = RandomForestRegressor(200)\n",
        "  forest.fit(x_train_rf, y_train_rf)\n",
        "\n",
        "  data=x_test[:,:,0]\n",
        "  prediction_seqs = []\n",
        "  pre_win_num=int(len(data)/prediction_len)\n",
        "  window_size=sequence_length\n",
        "\n",
        "  for i in range(0,pre_win_num):\n",
        "    curr_frame = data[i*prediction_len]\n",
        "    predicted = []\n",
        "    for j in range(0,prediction_len):\n",
        "      predicted.append(forest.predict(curr_frame[newaxis,:])[0])\n",
        "      curr_frame = curr_frame[1:]\n",
        "      curr_frame = np.insert(curr_frame, [window_size-2], predicted[-1], axis=0)\n",
        "    prediction_seqs.append(predicted)\n",
        "\n",
        "  #de_predicted\n",
        "  de_predicted=[]\n",
        "  len_pre_win=int(len(data)/prediction_len)\n",
        "  len_pre=prediction_len\n",
        "\n",
        "  m=0\n",
        "  for i in range(0,len_pre_win):\n",
        "    for j in range(0,len_pre):\n",
        "      de_predicted.append(prediction_seqs[i][j]*record_max[m]+record_min[m])\n",
        "      m=m+1\n",
        "\n",
        "  error = []\n",
        "  diff=y_test.shape[0]-prediction_len*pre_win_num\n",
        "\n",
        "  for i in range(y_test_ori.shape[0]-diff):\n",
        "      error.append(y_test_ori[i,] - de_predicted[i])\n",
        "\n",
        "  squaredError = []\n",
        "  absError = []\n",
        "  for val in error:\n",
        "      squaredError.append(val * val) \n",
        "      absError.append(abs(val))\n",
        "\n",
        "  error_percent=[]\n",
        "  for i in range(len(error)):\n",
        "    val=absError[i]/y_test_ori[i,]\n",
        "    val=abs(val)\n",
        "    error_percent.append(val)\n",
        "\n",
        "  mean_error_percent=sum(error_percent) / len(error_percent)\n",
        "  accuracy=1-mean_error_percent\n",
        "  \n",
        "  MSE=sum(squaredError) / len(squaredError)\n",
        "  return MSE,accuracy,y_test_ori,de_predicted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ef2oBPT9nJFH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filename=np.load('filename_delete_sort.npy')\n",
        "result_RF_df=pd.DataFrame(columns=('index','stock','MSE','accuracy','true','predict'))\n",
        "n=len(filename)\n",
        "                           \n",
        "for i in range(0,100):\n",
        "  index=i\n",
        "  stock=filename[i]\n",
        "  result=stock_RF_loop(filename)\n",
        "  MSE=result[0]\n",
        "  accuracy=result[1]\n",
        "  true=result[2]\n",
        "  predict=result[3]\n",
        "  result_RF_df=result_RF_df.append(pd.DataFrame({'index':[index],\n",
        "                                                     'stock':[stock],\n",
        "                                                     'MSE':[MSE],\n",
        "                                                     'accuracy':[accuracy],\n",
        "                                                     'true':[true],\n",
        "                                                     'predict':[predict]}),ignore_index=True)\n",
        "  print(i)\n",
        "  np.save('RF_451.npy',result_RF_df)\n",
        "  result_RF_df.to_csv('RF_d0.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wQNRadJpnXH2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Reshape data\n",
        "L4_all=np.load('RF_451.npy')\n",
        "L4_all=pd.DataFrame(L4_all)\n",
        "L4_all.columns=['MSE','accuracy','index','predict','stock','TRUE']\n",
        "\n",
        "#accuracy only\n",
        "filename=np.load('filename_delete_sort.npy')\n",
        "n=len(filename)\n",
        "len_pre=360\n",
        "\n",
        "result_df=pd.DataFrame(columns=('index','stock','TRUE','predict','accuracy','MSE'))\n",
        "for i in range(0,n):\n",
        "  index=i\n",
        "  stock=filename[i]\n",
        "  #TRUE\n",
        "  t=[]\n",
        "  for j in range(0,len_pre):\n",
        "    t.append(L4_all['TRUE'][i][j][0])\n",
        "  TRUE=t\n",
        "  #predict\n",
        "  t=[]\n",
        "  for j in range(0,len_pre):\n",
        "    t.append(L4_all['predict'][i][j])\n",
        "  predict=t\n",
        "  #accuracy\n",
        "  accuracy=[]\n",
        "  for j in range(0,len_pre):\n",
        "    t=abs(TRUE[j]-predict[j])/TRUE[j]\n",
        "    t1=1-t\n",
        "    accuracy.append(t1)\n",
        "  accuracy=accuracy\n",
        "  #MSE\n",
        "  MSE=L4_all['MSE'][i][0]\n",
        "  result_df=result_df.append(pd.DataFrame({'index':index,'stock':[stock],\n",
        "                                           'TRUE':[TRUE],\n",
        "                                           'predict':[predict],\n",
        "                                          'accuracy':[accuracy],\n",
        "                                          'MSE':[MSE]}),\n",
        "                             ignore_index=True)\n",
        "  print(i)\n",
        "np.save('RF_r.npy',result_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z70J49JBrj-y",
        "colab_type": "code",
        "outputId": "f1db77e7-01fb-4ef2-89ba-0954079e1571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "##Mean MPA of all stocks\n",
        "RF_r=np.load('RF_r.npy')\n",
        "RF_r=pd.DataFrame(RF_r)\n",
        "RF_r.columns=['MSE','TRUE','accuracy','index','predict','stock']\n",
        "\n",
        "n=451\n",
        "avg_accuracy1=[]\n",
        "for i in range(0,360):\n",
        "  t1=0\n",
        "  for j in range(0,n):\n",
        "    t1=t1+RF_r['accuracy'][j][i]\n",
        "  t1=t1/n\n",
        "  avg_accuracy1.append(t1)\n",
        "\n",
        "half1=[]\n",
        "for i in range(0,6): \n",
        "  half1.extend(avg_accuracy1[60*(i+1)-30:60*(i+1)]) \n",
        "mean1=pd.DataFrame(half1).mean()[0]\n",
        "\n",
        "print('Random forest Mean MPA:',mean1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random forest Mean MPA: 0.9234793321990546\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "taPz1Q31D7EF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Portfolio allocation"
      ]
    },
    {
      "metadata": {
        "id": "g5odtphXD9Bg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Mean variance portfolio allocation based on random forest  (0-60 days) , asset 1."
      ]
    },
    {
      "metadata": {
        "id": "k9-HraETEF8-",
        "colab_type": "code",
        "outputId": "fac345d0-8558-41e2-a613-717bc7ab3cff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "cell_type": "code",
      "source": [
        "LH_dph=np.load('RF_r.npy')\n",
        "LH_dph=pd.DataFrame(LH_dph)\n",
        "LH_dph.columns = ['MSE','TRUE','accuracy','index','predict','stock']\n",
        "\n",
        "test_win=6\n",
        "pre_len1=60\n",
        "pre_len2=60\n",
        "stock_len=451\n",
        "rf=0.015#risk free rate\n",
        "\n",
        "df_all=pd.DataFrame(columns=('index','return_pre','variance_pre','sharp_pre',\n",
        "                            'return_true','variance_true','sharp_true',))\n",
        "\n",
        "filename=np.load('filename_delete_sort.npy')\n",
        "df = pd.DataFrame()\n",
        "\n",
        "####cumulative\n",
        "for k in range(0,test_win):\n",
        "  n=stock_len\n",
        "  \n",
        "  for i in range(0,n):\n",
        "    t=LH_dph['predict'][i][k*pre_len1:k*pre_len1+pre_len1]\n",
        "    t1=[]\n",
        "    for j in range(0,pre_len1):\n",
        "      t1.append(t[j])\n",
        "    t=t1\n",
        "    df[filename[i]]=t  \n",
        "  data1=df\n",
        "  log_returns = np.log(data1 / data1.shift(1))\n",
        "  ret_index = (1 + log_returns).cumprod()\n",
        "  \n",
        "  choose_name=[]\n",
        "  choose_index=[]\n",
        "  for i in range(0,n):\n",
        "    if ret_index[filename[i]][59]>1.15:\n",
        "      choose_name.append(filename[i])\n",
        "      choose_index.append(i)\n",
        "      \n",
        "  #choose data\n",
        "  m=len(choose_index)\n",
        "  data2=data1.iloc[:,choose_index]\n",
        "  log_returns = np.log(data2 / data2.shift(1))\n",
        "  \n",
        "  rets = log_returns\n",
        "  year_ret = rets.mean() * 252\n",
        "  year_volatility = rets.cov() * 252\n",
        "  number_of_assets = m\n",
        "  weights = np.random.random(number_of_assets)\n",
        "  weights /= np.sum(weights)\n",
        "  \n",
        "  def statistics(weights):        \n",
        "    weights = np.array(weights)\n",
        "    pret = np.sum(rets.mean() * weights) * 252\n",
        "    pvol = np.sqrt(np.dot(weights.T, np.dot(rets.cov() * 252, weights)))\n",
        "    return np.array([pret, pvol, (pret-rf) / pvol])\n",
        "  \n",
        "  def min_func_sharpe(weights):\n",
        "    return -statistics(weights)[2]\n",
        "    \n",
        "  bnds = tuple((0,1) for x in range(number_of_assets))\n",
        "  cons = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
        "\n",
        "  opts = sco.minimize(min_func_sharpe, \n",
        "                      number_of_assets * [1. / number_of_assets,],\n",
        "                      method='SLSQP', \n",
        "                      bounds=bnds, \n",
        "                      constraints=cons)\n",
        "  \n",
        "  weights_pre=opts['x']\n",
        "  \n",
        "  ##check return\n",
        "  df2 = pd.DataFrame()\n",
        "  for i in choose_index:\n",
        "    t=LH_dph['TRUE'][i][k*pre_len1:k*pre_len1+pre_len1]\n",
        "    t1=[]\n",
        "    for j in range(0,pre_len1):\n",
        "      t1.append(t[j])\n",
        "    t= t1\n",
        "    df2['ture'+filename[i]]=t\n",
        "  data3=df2 \n",
        "  log_returns_true = np.log(data3 / data3.shift(1))\n",
        "  \n",
        "  rets_true = log_returns_true\n",
        "  year_ret_true = rets_true.mean() * 252\n",
        "  year_volatility_true = rets_true.cov() * 252\n",
        "  number_of_assets = m  #real asset number\n",
        "  \n",
        "  def statistics_true(weights):        \n",
        "    weights = np.array(weights)\n",
        "    pret = np.sum(rets_true.mean() * weights) * 252\n",
        "    pvol = np.sqrt(np.dot(weights.T, np.dot(rets_true.cov() * 252, weights)))\n",
        "    return np.array([pret, pvol, (pret-rf) / pvol])\n",
        "  \n",
        "  index=k\n",
        "  return_pre=statistics(opts['x'])[0]\n",
        "  variance_pre=statistics(opts['x'])[1]\n",
        "  sharp_pre=statistics(opts['x'])[2]\n",
        "  return_true=statistics_true(opts['x'])[0]\n",
        "  variance_true=statistics_true(opts['x'])[1]\n",
        "  sharp_true=statistics_true(opts['x'])[2]\n",
        "  \n",
        "  df_all=df_all.append(pd.DataFrame({'index':[index],\n",
        "                                     'return_pre':[return_pre],\n",
        "                                     'variance_pre':[variance_pre],\n",
        "                                     'sharp_pre':[sharp_pre],\n",
        "                                     'return_true':[ return_true],\n",
        "                                     'variance_true':[variance_true],\n",
        "                                    'sharp_true':[sharp_true],}),ignore_index=True)\n",
        "#   print('stock number:',n)\n",
        "#   print('count',k)\n",
        "#   print('choose number',m)\n",
        "#   print('initial random weight',weights)\n",
        "#   print('pre weights',opts['x'])\n",
        "print('Mean variance portfolio allocation based on random forest (0-60 days) , all stocks: \\n',df_all)\n",
        "#   print('weight',opts['x'].round(3))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean variance portfolio allocation based on random forest (0-60 days) , all stocks: \n",
            "   index  return_pre  return_true  sharp_pre  sharp_true  variance_pre  \\\n",
            "0     0    0.717399     0.817191  19.659196    7.510655      0.035729   \n",
            "1     1    0.871089     1.020737  28.125250   10.753563      0.030438   \n",
            "2     2    0.798626     0.591375  13.294077    1.951168      0.058945   \n",
            "3     3    0.757300     0.686891  41.640984    4.695748      0.017826   \n",
            "4     4    0.844309     0.765778  28.160862    8.022221      0.029449   \n",
            "5     5    1.150666     1.190725   6.933269    3.026948      0.163799   \n",
            "\n",
            "   variance_true  \n",
            "0       0.106807  \n",
            "1       0.093526  \n",
            "2       0.295400  \n",
            "3       0.143085  \n",
            "4       0.093587  \n",
            "5       0.388419  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ggVSQIhCE9-p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Minimum variance portfolio allocation based on random forest (0-60 days) , asset 1."
      ]
    },
    {
      "metadata": {
        "id": "LiMVf9k_FC5D",
        "colab_type": "code",
        "outputId": "68211f5a-6fcf-4ac9-f97e-e44eb8f44674",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "cell_type": "code",
      "source": [
        "LH_dph=np.load('RF_r.npy')\n",
        "LH_dph=pd.DataFrame(LH_dph)\n",
        "LH_dph.columns = ['MSE','TRUE','accuracy','index','predict','stock']\n",
        "\n",
        "test_win=6\n",
        "pre_len1=60\n",
        "pre_len2=60\n",
        "stock_len=451\n",
        "rf=0.015#risk free rate\n",
        "\n",
        "df_all=pd.DataFrame(columns=('index','return_pre','variance_pre','sharp_pre',\n",
        "                            'return_true','variance_true','sharp_true',))\n",
        "\n",
        "filename=np.load('filename_delete_sort.npy')\n",
        "df = pd.DataFrame()\n",
        "\n",
        "####cumulative\n",
        "for k in range(0,test_win):\n",
        "  n=stock_len\n",
        "  \n",
        "  for i in range(0,n):\n",
        "    t=LH_dph['predict'][i][k*pre_len1:k*pre_len1+pre_len1]\n",
        "    t1=[]\n",
        "    for j in range(0,pre_len1):\n",
        "      t1.append(t[j])\n",
        "    t=t1\n",
        "    df[filename[i]]=t  \n",
        "  data1=df\n",
        "  log_returns = np.log(data1 / data1.shift(1))\n",
        "  ret_index = (1 + log_returns).cumprod()\n",
        "  \n",
        "  choose_name=[]\n",
        "  choose_index=[]\n",
        "  for i in range(0,n):\n",
        "    if ret_index[filename[i]][59]>1.15:\n",
        "      choose_name.append(filename[i])\n",
        "      choose_index.append(i)\n",
        "      \n",
        "  #choose data\n",
        "  m=len(choose_index)\n",
        "  data2=data1.iloc[:,choose_index]\n",
        "  log_returns = np.log(data2 / data2.shift(1))\n",
        "  \n",
        "  rets = log_returns\n",
        "  year_ret = rets.mean() * 252\n",
        "  year_volatility = rets.cov() * 252\n",
        "  number_of_assets = m\n",
        "  weights = np.random.random(number_of_assets)\n",
        "  weights /= np.sum(weights)\n",
        "  \n",
        "  def statistics(weights):        \n",
        "    weights = np.array(weights)\n",
        "    pret = np.sum(rets.mean() * weights) * 252\n",
        "    pvol = np.sqrt(np.dot(weights.T, np.dot(rets.cov() * 252, weights)))\n",
        "    return np.array([pret, pvol, (pret-rf) / pvol])\n",
        "  \n",
        "  def min_func_sharpe(weights):\n",
        "    return statistics(weights)[1]\n",
        "    \n",
        "  bnds = tuple((0,1) for x in range(number_of_assets))\n",
        "  cons = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
        "\n",
        "  opts = sco.minimize(min_func_sharpe, \n",
        "                      number_of_assets * [1. / number_of_assets,],\n",
        "                      method='SLSQP', \n",
        "                      bounds=bnds, \n",
        "                      constraints=cons)\n",
        "  \n",
        "  weights_pre=opts['x']\n",
        "  \n",
        "  ##check return\n",
        "  df2 = pd.DataFrame()\n",
        "  for i in choose_index:\n",
        "    t=LH_dph['TRUE'][i][k*pre_len1:k*pre_len1+pre_len1]\n",
        "    t1=[]\n",
        "    for j in range(0,pre_len1):\n",
        "      t1.append(t[j])\n",
        "    t= t1\n",
        "    df2['ture'+filename[i]]=t\n",
        "  data3=df2 \n",
        "  log_returns_true = np.log(data3 / data3.shift(1))\n",
        "  \n",
        "  rets_true = log_returns_true\n",
        "  year_ret_true = rets_true.mean() * 252\n",
        "  year_volatility_true = rets_true.cov() * 252\n",
        "  number_of_assets = m  #real asset number\n",
        "  \n",
        "  def statistics_true(weights):        \n",
        "    weights = np.array(weights)\n",
        "    pret = np.sum(rets_true.mean() * weights) * 252\n",
        "    pvol = np.sqrt(np.dot(weights.T, np.dot(rets_true.cov() * 252, weights)))\n",
        "    return np.array([pret, pvol, (pret-rf) / pvol])\n",
        "  \n",
        "  index=k\n",
        "  return_pre=statistics(opts['x'])[0]\n",
        "  variance_pre=statistics(opts['x'])[1]\n",
        "  sharp_pre=statistics(opts['x'])[2]\n",
        "  return_true=statistics_true(opts['x'])[0]\n",
        "  variance_true=statistics_true(opts['x'])[1]\n",
        "  sharp_true=statistics_true(opts['x'])[2]\n",
        "  \n",
        "  df_all=df_all.append(pd.DataFrame({'index':[index],\n",
        "                                     'return_pre':[return_pre],\n",
        "                                     'variance_pre':[variance_pre],\n",
        "                                     'sharp_pre':[sharp_pre],\n",
        "                                     'return_true':[ return_true],\n",
        "                                     'variance_true':[variance_true],\n",
        "                                    'sharp_true':[sharp_true],}),ignore_index=True)\n",
        "#   print('stock number:',n)\n",
        "#   print('count',k)\n",
        "#   print('choose number',m)\n",
        "#   print('initial random weight',weights)\n",
        "#   print('pre weights',opts['x'])\n",
        "print('Minimum variance portfolio allocation based on random forest (0-60 days) , all stocks: \\n',df_all)\n",
        "#   print('weight',opts['x'].round(3))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Minimum variance portfolio allocation based on random forest (0-60 days) , all stocks: \n",
            "   index  return_pre  return_true  sharp_pre  sharp_true  variance_pre  \\\n",
            "0     0    0.680273     0.781484  19.155383    7.872169      0.034730   \n",
            "1     1    0.784519     0.929849  26.711894    9.670705      0.028808   \n",
            "2     2    0.750628     0.530903  12.889213    1.771770      0.057073   \n",
            "3     3    0.728280     0.636056  40.807891    4.330639      0.017479   \n",
            "4     4    0.746873     0.698547  26.706671    7.516446      0.027404   \n",
            "5     5    1.129279     1.181166   6.867512    2.896046      0.162254   \n",
            "\n",
            "   variance_true  \n",
            "0       0.097366  \n",
            "1       0.094600  \n",
            "2       0.291179  \n",
            "3       0.143410  \n",
            "4       0.090940  \n",
            "5       0.402675  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-6vrf8tpFu0E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Mean variance portfolio allocation based on random forest (0-60 days) , asset 2."
      ]
    },
    {
      "metadata": {
        "id": "PZapscYSFvmv",
        "colab_type": "code",
        "outputId": "7f26431a-8029-4cab-9ed5-acc43cbd676e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "cell_type": "code",
      "source": [
        "filename=np.load('filename_delete_sort.npy')\n",
        "sort=np.load('new_sortname.npy')\n",
        "\n",
        "LH_dph=np.load('RF_r.npy')\n",
        "LH_dph=pd.DataFrame(LH_dph)\n",
        "LH_dph.columns = ['MSE','TRUE','accuracy','index','predict','stock']\n",
        "LH_dph_ori=LH_dph\n",
        "\n",
        "top=50\n",
        "\n",
        "LH_dph=[]\n",
        "for i in range(0,451):\n",
        "  for j in range(0,top):\n",
        "    if LH_dph_ori['stock'][i]==sort[j]:\n",
        "      LH_dph.append(LH_dph_ori.iloc[i,])\n",
        "      \n",
        "LH_dph=np.array(LH_dph)\n",
        "LH_dph=pd.DataFrame(LH_dph)\n",
        "LH_dph.columns = ['MSE','TRUE','accuracy','index','predict','stock']\n",
        "\n",
        "test_win=6\n",
        "pre_len1=60\n",
        "pre_len2=60\n",
        "rf=0.015\n",
        "stock_len=top\n",
        "\n",
        "df_all=pd.DataFrame(columns=('index','return_pre','variance_pre','sharp_pre',\n",
        "                            'return_true','variance_true','sharp_true',))\n",
        "\n",
        "filename=np.load('filename_delete_sort.npy')\n",
        "df = pd.DataFrame()\n",
        "\n",
        "rets_true_all=pd.DataFrame()\n",
        "####cumulative\n",
        "for k in range(0,test_win):\n",
        "  n=stock_len\n",
        "  \n",
        "  for i in range(0,n):\n",
        "    t=LH_dph['predict'][i][k*pre_len1:k*pre_len1+pre_len1]\n",
        "    t1=[]\n",
        "    for j in range(0,pre_len1):\n",
        "      t1.append(t[j])\n",
        "    t=t1\n",
        "    df[filename[i]]=t  \n",
        "  data1=df\n",
        "  log_returns = np.log(data1 / data1.shift(1))\n",
        "  ret_index = (1 + log_returns).cumprod()\n",
        "  \n",
        "  choose_name=[]\n",
        "  choose_index=[]\n",
        "  for i in range(0,n):\n",
        "    if ret_index[filename[i]][59]>1.03:\n",
        "        choose_name.append(filename[i])\n",
        "        choose_index.append(i)\n",
        "\n",
        "  #choose data\n",
        "  m=len(choose_index)\n",
        "  data2=data1.iloc[:,choose_index]\n",
        "  log_returns = np.log(data2 / data2.shift(1))\n",
        "  \n",
        "  rets = log_returns\n",
        "  number_of_assets = m\n",
        "  weights = np.random.random(number_of_assets)\n",
        "  weights /= np.sum(weights)\n",
        "  \n",
        "  def statistics(weights):        \n",
        "    weights = np.array(weights)\n",
        "    pret = np.sum(rets.mean() * weights) * 252\n",
        "    pvol = np.sqrt(np.dot(weights.T, np.dot(rets.cov() * 252, weights)))\n",
        "    return np.array([pret, pvol, (pret-rf) / pvol])\n",
        "  \n",
        "  def min_func_sharpe(weights):\n",
        "    return -statistics(weights)[2]\n",
        "  \n",
        "  bnds = tuple((0,1) for x in range(number_of_assets))\n",
        "  cons = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
        "\n",
        "  opts = sco.minimize(min_func_sharpe, \n",
        "                      number_of_assets * [1. / number_of_assets,],\n",
        "                      method='SLSQP', \n",
        "                      bounds=bnds, \n",
        "                      constraints=cons)\n",
        "  \n",
        "  weights_pre=opts['x']\n",
        "  \n",
        "  ##check return\n",
        "  df2 = pd.DataFrame()\n",
        "  for i in choose_index:\n",
        "    t=LH_dph['TRUE'][i][k*pre_len1:k*pre_len1+pre_len1]\n",
        "    t1=[]\n",
        "    for j in range(0,pre_len1):\n",
        "      t1.append(t[j])\n",
        "    t= t1\n",
        "    df2['ture'+filename[i]]=t\n",
        "  data3=df2 \n",
        "  log_returns_true = np.log(data3 / data3.shift(1))\n",
        "  \n",
        "  rets_true = log_returns_true\n",
        "  a=rets_true_all.append(rets_true,ignore_index=True) \n",
        "  number_of_assets = m  #real asset number\n",
        "  \n",
        "  def statistics_true(weights):        \n",
        "    weights = np.array(weights)\n",
        "    pret = np.sum(rets_true.mean() * weights) * 252\n",
        "    pvol = np.sqrt(np.dot(weights.T, np.dot(rets_true.cov() * 252, weights)))\n",
        "    return np.array([pret, pvol, (pret-rf) / pvol])\n",
        "  \n",
        "  index=k\n",
        "  return_pre=statistics(opts['x'])[0]\n",
        "  variance_pre=statistics(opts['x'])[1]\n",
        "  sharp_pre=statistics(opts['x'])[2]\n",
        "  return_true=statistics_true(opts['x'])[0]\n",
        "  variance_true=statistics_true(opts['x'])[1]\n",
        "  sharp_true=statistics_true(opts['x'])[2]\n",
        "  \n",
        "  df_all=df_all.append(pd.DataFrame({'index':[index],\n",
        "                                     'return_pre':[return_pre],\n",
        "                                     'variance_pre':[variance_pre],\n",
        "                                     'sharp_pre':[sharp_pre],\n",
        "                                     'return_true':[ return_true],\n",
        "                                     'variance_true':[variance_true],\n",
        "                                    'sharp_true':[sharp_true],}),ignore_index=True)\n",
        "#   print('stock number:',n)\n",
        "#   print('count',k)\n",
        "#   print('choose number',m)\n",
        "#   print('initial random weight',weights)\n",
        "#   print('pre weights',opts['x'])\n",
        "#   print('weight',opts['x'].round(3))\n",
        "print('Mean variance portfolio allocation based on random forest (0-60 days) , asset 2: \\n',df_all)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean variance portfolio allocation based on random forest (0-60 days) , asset 2: \n",
            "   index  return_pre  return_true  sharp_pre  sharp_true  variance_pre  \\\n",
            "0     0    0.224186     0.257846  19.362971    3.795400      0.010803   \n",
            "1     1    0.273187     0.325342  15.254361    4.971377      0.016925   \n",
            "2     2    0.365133     0.104958  13.471132    0.457251      0.025991   \n",
            "3     3    0.323000     0.225800  24.074160    1.769487      0.012794   \n",
            "4     4    0.329225     0.336975  32.799593    4.395588      0.009580   \n",
            "5     5    0.245199     0.140088  14.109972    0.804220      0.016315   \n",
            "\n",
            "   variance_true  \n",
            "0       0.063984  \n",
            "1       0.062426  \n",
            "2       0.196737  \n",
            "3       0.119131  \n",
            "4       0.073250  \n",
            "5       0.155540  \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}