{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "EymK2QvOTUBt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# LSTM main files\n",
        "Code editor: Xinyi Li, Yinchuan Li. Date: 2019.2.20.\n",
        "\n",
        "The code is run on Google's Colaboratory with Python 3.\n",
        "\n",
        "Paper: Mid-LSTM meets Mid-ARMA: deep learning for midterm stock prediction."
      ]
    },
    {
      "metadata": {
        "id": "elQ2sRI4sKCk",
        "colab_type": "code",
        "outputId": "144086b7-252f-4078-9e2c-7358d2d004c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "\n",
        "import os\n",
        "os.chdir(\"drive/download data/sp500new\")\n",
        "# !ls\n",
        "from google.colab import files\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "from numpy import newaxis\n",
        "from keras.layers import Dense, Activation, Dropout, LSTM\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "from math import pi,sqrt,exp,pow,log\n",
        "from numpy.linalg import det, inv\n",
        "from abc import ABCMeta, abstractmethod\n",
        "from sklearn import cluster\n",
        "\n",
        "import statsmodels.api as sm \n",
        "import scipy.stats as scs\n",
        "import scipy.optimize as sco\n",
        "import scipy.interpolate as sci\n",
        "from scipy import stats"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 131322 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.1-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
            "  from pandas.core import datetools\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "3a-X7k0tscuB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def stock_loop (filename):\n",
        "  split = (0.85);\n",
        "  sequence_length=60;\n",
        "  normalise= True\n",
        "  batch_size=60;\n",
        "  input_dim=1\n",
        "  input_timesteps=59\n",
        "  neurons=60\n",
        "  epochs=2\n",
        "  prediction_len=60\n",
        "  dense_output=1\n",
        "  drop_out=0.2\n",
        "  \n",
        "  dataframe = pd.read_csv(filename)\n",
        "  cols = ['Close_y']\n",
        "  \n",
        "  len_dataframe=dataframe.shape[0]\n",
        "\n",
        "  i_split = int(len(dataframe) * split)\n",
        "  data_train = dataframe.get(cols).values[:i_split]\n",
        "  data_test  = dataframe.get(cols).values[i_split:]\n",
        "  len_train  = len(data_train)\n",
        "  len_test   = len(data_test)\n",
        "  len_train_windows = None\n",
        "  \n",
        "  #get_test_data    ############################################################\n",
        "  data_windows = []\n",
        "  for i in range(len_test - sequence_length):\n",
        "    data_windows.append(data_test[i:i+sequence_length])\n",
        "  data_windows = np.array(data_windows).astype(float)\n",
        "  # get original y_test\n",
        "  y_test_ori = data_windows[:, -1, [0]]\n",
        "  \n",
        "  window_data=data_windows\n",
        "  win_num=window_data.shape[0]\n",
        "  col_num=window_data.shape[2]\n",
        "  normalised_data = []\n",
        "  record_min=[]\n",
        "  record_max=[]\n",
        "  \n",
        "  #normalize\n",
        "  for win_i in range(0,win_num):\n",
        "    normalised_window = []\n",
        "    for col_i in range(0,col_num):\n",
        "      temp_col=window_data[win_i,:,col_i]\n",
        "      temp_min=min(temp_col)\n",
        "      if col_i==0:\n",
        "        record_min.append(temp_min)#record min\n",
        "      temp_col=temp_col-temp_min\n",
        "      temp_max=max(temp_col)\n",
        "      if col_i==0:\n",
        "        record_max.append(temp_max)#record max\n",
        "      temp_col=temp_col/temp_max\n",
        "      normalised_window.append(temp_col)\n",
        "    normalised_window = np.array(normalised_window).T\n",
        "    normalised_data.append(normalised_window)\n",
        "  normalised_data=np.array(normalised_data)\n",
        "  \n",
        "  data_windows=normalised_data#get_test_data\n",
        "  x_test = data_windows[:, :-1]\n",
        "  y_test = data_windows[:, -1, [0]]\n",
        "\n",
        "  #get_train_data ##############################################################\n",
        "  data_windows = []\n",
        "  for i in range(len_train - sequence_length):\n",
        "    data_windows.append(data_train[i:i+sequence_length])\n",
        "  data_windows = np.array(data_windows).astype(float)\n",
        "  \n",
        "  window_data=data_windows\n",
        "  win_num=window_data.shape[0]\n",
        "  col_num=window_data.shape[2]\n",
        "  \n",
        "  normalised_data = []\n",
        "  for win_i in range(0,win_num):\n",
        "    normalised_window = []\n",
        "    for col_i in range(0,col_num):\n",
        "      temp_col=window_data[win_i,:,col_i]\n",
        "      temp_min=min(temp_col)\n",
        "      temp_col=temp_col-temp_min\n",
        "      temp_max=max(temp_col)\n",
        "      temp_col=temp_col/temp_max\n",
        "      normalised_window.append(temp_col)\n",
        "    normalised_window = np.array(normalised_window).T\n",
        "    normalised_data.append(normalised_window)\n",
        "  normalised_data=np.array(normalised_data)\n",
        "  \n",
        "  data_windows=normalised_data\n",
        "  x_train = data_windows[:, :-1]\n",
        "  y_train = data_windows[:, -1]\n",
        "  \n",
        "  # LSTM MODEL\n",
        "  # create model\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(neurons, input_shape=(input_timesteps, input_dim), return_sequences = True))\n",
        "  model.add(Dropout(drop_out))\n",
        "  model.add(LSTM(neurons,return_sequences = True))\n",
        "  model.add(LSTM(neurons,return_sequences =False))\n",
        "  model.add(Dropout(drop_out))\n",
        "  model.add(Dense(dense_output, activation='linear'))\n",
        "  # Compile model\n",
        "  model.compile(loss='mean_squared_error',\n",
        "                optimizer='adam')\n",
        "  # Fit the model\n",
        "  model.fit(x_train,y_train,epochs=epochs,batch_size=batch_size)\n",
        "  \n",
        "  #multi sequence predict\n",
        "  data=x_test\n",
        "  prediction_seqs = []\n",
        "  window_size=sequence_length\n",
        "  pre_win_num=int(len(data)/prediction_len)\n",
        " \n",
        "  for i in range(0,pre_win_num):\n",
        "    curr_frame = data[i*prediction_len]\n",
        "    predicted = []\n",
        "    for j in range(0,prediction_len):\n",
        "      temp=model.predict(curr_frame[newaxis,:,:])[0]\n",
        "      predicted.append(temp)\n",
        "      curr_frame = curr_frame[1:]\n",
        "      curr_frame = np.insert(curr_frame, [window_size-2], predicted[-1], axis=0)\n",
        "    prediction_seqs.append(predicted)\n",
        "  \n",
        "  #de_predicted\n",
        "  de_predicted=[]\n",
        "  len_pre_win=int(len(data)/prediction_len)\n",
        "  len_pre=prediction_len\n",
        "\n",
        "  m=0\n",
        "  for i in range(0,len_pre_win):\n",
        "    for j in range(0,len_pre):\n",
        "      de_predicted.append(prediction_seqs[i][j][0]*record_max[m]+record_min[m])\n",
        "      m=m+1\n",
        "  \n",
        "  error = []\n",
        "  diff=y_test.shape[0]-prediction_len*pre_win_num\n",
        "\n",
        "  for i in range(y_test_ori.shape[0]-diff):\n",
        "      error.append(y_test_ori[i,] - de_predicted[i])\n",
        "    \n",
        "  squaredError = []\n",
        "  absError = []\n",
        "  for val in error:\n",
        "      squaredError.append(val * val) \n",
        "      absError.append(abs(val))\n",
        "\n",
        "  error_percent=[]\n",
        "  for i in range(len(error)):\n",
        "    val=absError[i]/y_test_ori[i,]\n",
        "    val=abs(val)\n",
        "    error_percent.append(val)\n",
        "\n",
        "  mean_error_percent=sum(error_percent) / len(error_percent)\n",
        "  accuracy=1-mean_error_percent\n",
        "\n",
        "  MSE=sum(squaredError) / len(squaredError)\n",
        "  return MSE,accuracy,y_test_ori,de_predicted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dj6WOr9pswrG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filename=np.load('filename_delete_sort.npy')\n",
        "\n",
        "result_LSTM_df=pd.DataFrame(columns=('index','stock','MSE','accuracy','true','predict'))\n",
        "n=len(filename)\n",
        "                           \n",
        "for i in range(416,n):\n",
        "  index=i\n",
        "  stock=filename[i]\n",
        "  result=stock_loop(filename[i])\n",
        "  MSE=result[0]\n",
        "  accuracy=result[1]\n",
        "  true=result[2]\n",
        "  predict=result[3]\n",
        "  result_LSTM_df=result_LSTM_df.append(pd.DataFrame({'index':[index],\n",
        "                                                     'stock':[stock],\n",
        "                                                     'MSE':[MSE],\n",
        "                                                     'accuracy':[accuracy],\n",
        "                                                     'true':[true],\n",
        "                                                     'predict':[predict]}),ignore_index=True)\n",
        "  print(i)\n",
        "  np.save('L1_451.npy',result_LSTM_df)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1BNbGMcrs61A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Reshape data\n",
        "L4_all=np.load('L1_451.npy')\n",
        "L4_all=pd.DataFrame(L4_all)\n",
        "L4_all.columns=['MSE','accuracy','index','predict','stock','TRUE']\n",
        "\n",
        "#accuracy only\n",
        "filename=np.load('filename_delete_sort.npy')\n",
        "n=len(filename)\n",
        "len_pre=360\n",
        "\n",
        "result_df=pd.DataFrame(columns=('index','stock','TRUE','predict','accuracy','MSE'))\n",
        "for i in range(0,n):\n",
        "  index=i\n",
        "  stock=filename[i]\n",
        "  #TRUE\n",
        "  t=[]\n",
        "  for j in range(0,len_pre):\n",
        "    t.append(L4_all['TRUE'][i][j][0])\n",
        "  TRUE=t\n",
        "  #predict\n",
        "  t=[]\n",
        "  for j in range(0,len_pre):\n",
        "    t.append(L4_all['predict'][i][j])\n",
        "  predict=t\n",
        "  #accuracy\n",
        "  accuracy=[]\n",
        "  for j in range(0,len_pre):\n",
        "    t=abs(TRUE[j]-predict[j])/TRUE[j]\n",
        "    t1=1-t\n",
        "    accuracy.append(t1)\n",
        "  accuracy=accuracy\n",
        "  #MSE\n",
        "  MSE=L4_all['MSE'][i][0]\n",
        "  result_df=result_df.append(pd.DataFrame({'index':index,'stock':[stock],\n",
        "                                           'TRUE':[TRUE],\n",
        "                                           'predict':[predict],\n",
        "                                          'accuracy':[accuracy],\n",
        "                                          'MSE':[MSE]}),\n",
        "                             ignore_index=True)\n",
        "  print(i)\n",
        "np.save('L1_r.npy',result_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vWqrXI1otONI",
        "colab_type": "code",
        "outputId": "cffb7813-eef0-46c1-d086-3218407488c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "##Mean MPA of all stocks\n",
        "L1_r=np.load('L1_r.npy')\n",
        "L1_r=pd.DataFrame(L1_r)\n",
        "L1_r.columns=['MSE','TRUE','accuracy','index','predict','stock']\n",
        "\n",
        "n=451\n",
        "avg_accuracy1=[]\n",
        "for i in range(0,360):\n",
        "  t1=0\n",
        "  for j in range(0,n):\n",
        "    t1=t1+L1_r['accuracy'][j][i]\n",
        "  t1=t1/n\n",
        "  avg_accuracy1.append(t1)\n",
        "\n",
        "half1=[]\n",
        "for i in range(0,6): \n",
        "  half1.extend(avg_accuracy1[60*(i+1)-30:60*(i+1)]) \n",
        "mean1=pd.DataFrame(half1).mean()[0]\n",
        "\n",
        "print('LSTM Mean MPA:',mean1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTM Mean MPA: 0.9257633021660404\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0MbMrnmQJLWO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Portfolio allocation"
      ]
    },
    {
      "metadata": {
        "id": "i1RMyeXfJM9E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Mean variance portfolio allocation based on LSTM (0-60 days) , asset 1."
      ]
    },
    {
      "metadata": {
        "id": "oWEbpDfiJNsx",
        "colab_type": "code",
        "outputId": "d6fab38a-6393-42f8-efe4-ab21d6310f46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "cell_type": "code",
      "source": [
        "LH_dph=np.load('L1_r.npy')\n",
        "LH_dph=pd.DataFrame(LH_dph)\n",
        "LH_dph.columns = ['MSE','TRUE','accuracy','index','predict','stock']\n",
        "\n",
        "test_win=6\n",
        "pre_len1=60\n",
        "pre_len2=60\n",
        "stock_len=451\n",
        "rf=0.015#risk free rate\n",
        "\n",
        "df_all=pd.DataFrame(columns=('index','return_pre','variance_pre','sharp_pre',\n",
        "                            'return_true','variance_true','sharp_true',))\n",
        "\n",
        "filename=np.load('filename_delete_sort.npy')\n",
        "df = pd.DataFrame()\n",
        "\n",
        "####cumulative\n",
        "for k in range(0,test_win):\n",
        "  n=stock_len\n",
        "  \n",
        "  for i in range(0,n):\n",
        "    t=LH_dph['predict'][i][k*pre_len1:k*pre_len1+pre_len1]\n",
        "    t1=[]\n",
        "    for j in range(0,pre_len1):\n",
        "      t1.append(t[j])\n",
        "    t=t1\n",
        "    df[filename[i]]=t  \n",
        "  data1=df\n",
        "  log_returns = np.log(data1 / data1.shift(1))\n",
        "  ret_index = (1 + log_returns).cumprod()\n",
        "  \n",
        "  choose_name=[]\n",
        "  choose_index=[]\n",
        "  for i in range(0,n):\n",
        "    if ret_index[filename[i]][59]>1.15:\n",
        "      choose_name.append(filename[i])\n",
        "      choose_index.append(i)\n",
        "      \n",
        "  #choose data\n",
        "  m=len(choose_index)\n",
        "  data2=data1.iloc[:,choose_index]\n",
        "  log_returns = np.log(data2 / data2.shift(1))\n",
        "  \n",
        "  rets = log_returns\n",
        "  year_ret = rets.mean() * 252\n",
        "  year_volatility = rets.cov() * 252\n",
        "  number_of_assets = m\n",
        "  weights = np.random.random(number_of_assets)\n",
        "  weights /= np.sum(weights)\n",
        "  \n",
        "  def statistics(weights):        \n",
        "    weights = np.array(weights)\n",
        "    pret = np.sum(rets.mean() * weights) * 252\n",
        "    pvol = np.sqrt(np.dot(weights.T, np.dot(rets.cov() * 252, weights)))\n",
        "    return np.array([pret, pvol, (pret-rf) / pvol])\n",
        "  \n",
        "  def min_func_sharpe(weights):\n",
        "    return -statistics(weights)[2]\n",
        "    \n",
        "  bnds = tuple((0,1) for x in range(number_of_assets))\n",
        "  cons = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
        "\n",
        "  opts = sco.minimize(min_func_sharpe, \n",
        "                      number_of_assets * [1. / number_of_assets,],\n",
        "                      method='SLSQP', \n",
        "                      bounds=bnds, \n",
        "                      constraints=cons)\n",
        "  \n",
        "  weights_pre=opts['x']\n",
        "  \n",
        "  ##check return\n",
        "  df2 = pd.DataFrame()\n",
        "  for i in choose_index:\n",
        "    t=LH_dph['TRUE'][i][k*pre_len1:k*pre_len1+pre_len1]\n",
        "    t1=[]\n",
        "    for j in range(0,pre_len1):\n",
        "      t1.append(t[j])\n",
        "    t= t1\n",
        "    df2['ture'+filename[i]]=t\n",
        "  data3=df2 \n",
        "  log_returns_true = np.log(data3 / data3.shift(1))\n",
        "  \n",
        "  rets_true = log_returns_true\n",
        "  year_ret_true = rets_true.mean() * 252\n",
        "  year_volatility_true = rets_true.cov() * 252\n",
        "  number_of_assets = m  #real asset number\n",
        "  \n",
        "  def statistics_true(weights):        \n",
        "    weights = np.array(weights)\n",
        "    pret = np.sum(rets_true.mean() * weights) * 252\n",
        "    pvol = np.sqrt(np.dot(weights.T, np.dot(rets_true.cov() * 252, weights)))\n",
        "    return np.array([pret, pvol, (pret-rf) / pvol])\n",
        "  \n",
        "  index=k\n",
        "  return_pre=statistics(opts['x'])[0]\n",
        "  variance_pre=statistics(opts['x'])[1]\n",
        "  sharp_pre=statistics(opts['x'])[2]\n",
        "  return_true=statistics_true(opts['x'])[0]\n",
        "  variance_true=statistics_true(opts['x'])[1]\n",
        "  sharp_true=statistics_true(opts['x'])[2]\n",
        "  \n",
        "  df_all=df_all.append(pd.DataFrame({'index':[index],\n",
        "                                     'return_pre':[return_pre],\n",
        "                                     'variance_pre':[variance_pre],\n",
        "                                     'sharp_pre':[sharp_pre],\n",
        "                                     'return_true':[ return_true],\n",
        "                                     'variance_true':[variance_true],\n",
        "                                    'sharp_true':[sharp_true],}),ignore_index=True)\n",
        "#   print('stock number:',n)\n",
        "#   print('count',k)\n",
        "#   print('choose number',m)\n",
        "#   print('initial random weight',weights)\n",
        "#   print('pre weights',opts['x'])\n",
        "print('Mean variance portfolio allocation based on LSTM (0-60 days) , all stocks: \\n',df_all)\n",
        "#   print('weight',opts['x'].round(3))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean variance portfolio allocation based on LSTM (0-60 days) , all stocks: \n",
            "   index  return_pre  return_true  sharp_pre  sharp_true  variance_pre  \\\n",
            "0     0    0.766890     0.691092  29.514717    5.564410      0.025475   \n",
            "1     1    0.828745     0.966165  32.477436   13.275569      0.025056   \n",
            "2     2    0.958448     0.650894  16.428647    2.420991      0.057427   \n",
            "3     3    0.770894     0.793108  38.301150    5.918399      0.019736   \n",
            "4     4    0.742025     0.742179  40.068889    7.699105      0.018144   \n",
            "5     5    0.690272    -0.962326  28.594022   -3.192486      0.023616   \n",
            "\n",
            "   variance_true  \n",
            "0       0.121503  \n",
            "1       0.071648  \n",
            "2       0.262659  \n",
            "3       0.131473  \n",
            "4       0.094450  \n",
            "5       0.306133  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-5dVdaB-LT3f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Minimum variance portfolio allocation based on LSTM (0-60 days) , asset 1."
      ]
    },
    {
      "metadata": {
        "id": "ebvabbZ1LYL3",
        "colab_type": "code",
        "outputId": "025879b3-6530-437f-a445-6f3d26270206",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "cell_type": "code",
      "source": [
        "LH_dph=np.load('L1_r.npy')\n",
        "LH_dph=pd.DataFrame(LH_dph)\n",
        "LH_dph.columns = ['MSE','TRUE','accuracy','index','predict','stock']\n",
        "\n",
        "test_win=6\n",
        "pre_len1=60\n",
        "pre_len2=60\n",
        "stock_len=451\n",
        "rf=0.015#risk free rate\n",
        "\n",
        "df_all=pd.DataFrame(columns=('index','return_pre','variance_pre','sharp_pre',\n",
        "                            'return_true','variance_true','sharp_true',))\n",
        "\n",
        "filename=np.load('filename_delete_sort.npy')\n",
        "df = pd.DataFrame()\n",
        "\n",
        "####cumulative\n",
        "for k in range(0,test_win):\n",
        "  n=stock_len\n",
        "  \n",
        "  for i in range(0,n):\n",
        "    t=LH_dph['predict'][i][k*pre_len1:k*pre_len1+pre_len1]\n",
        "    t1=[]\n",
        "    for j in range(0,pre_len1):\n",
        "      t1.append(t[j])\n",
        "    t=t1\n",
        "    df[filename[i]]=t  \n",
        "  data1=df\n",
        "  log_returns = np.log(data1 / data1.shift(1))\n",
        "  ret_index = (1 + log_returns).cumprod()\n",
        "  \n",
        "  choose_name=[]\n",
        "  choose_index=[]\n",
        "  for i in range(0,n):\n",
        "    if ret_index[filename[i]][59]>1.15:\n",
        "      choose_name.append(filename[i])\n",
        "      choose_index.append(i)\n",
        "      \n",
        "  #choose data\n",
        "  m=len(choose_index)\n",
        "  data2=data1.iloc[:,choose_index]\n",
        "  log_returns = np.log(data2 / data2.shift(1))\n",
        "  \n",
        "  rets = log_returns\n",
        "  year_ret = rets.mean() * 252\n",
        "  year_volatility = rets.cov() * 252\n",
        "  number_of_assets = m\n",
        "  weights = np.random.random(number_of_assets)\n",
        "  weights /= np.sum(weights)\n",
        "  \n",
        "  def statistics(weights):        \n",
        "    weights = np.array(weights)\n",
        "    pret = np.sum(rets.mean() * weights) * 252\n",
        "    pvol = np.sqrt(np.dot(weights.T, np.dot(rets.cov() * 252, weights)))\n",
        "    return np.array([pret, pvol, (pret-rf) / pvol])\n",
        "  \n",
        "  def min_func_sharpe(weights):\n",
        "    return statistics(weights)[1]\n",
        "    \n",
        "  bnds = tuple((0,1) for x in range(number_of_assets))\n",
        "  cons = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
        "\n",
        "  opts = sco.minimize(min_func_sharpe, \n",
        "                      number_of_assets * [1. / number_of_assets,],\n",
        "                      method='SLSQP', \n",
        "                      bounds=bnds, \n",
        "                      constraints=cons)\n",
        "  \n",
        "  weights_pre=opts['x']\n",
        "  \n",
        "  ##check return\n",
        "  df2 = pd.DataFrame()\n",
        "  for i in choose_index:\n",
        "    t=LH_dph['TRUE'][i][k*pre_len1:k*pre_len1+pre_len1]\n",
        "    t1=[]\n",
        "    for j in range(0,pre_len1):\n",
        "      t1.append(t[j])\n",
        "    t= t1\n",
        "    df2['ture'+filename[i]]=t\n",
        "  data3=df2 \n",
        "  log_returns_true = np.log(data3 / data3.shift(1))\n",
        "  \n",
        "  rets_true = log_returns_true\n",
        "  year_ret_true = rets_true.mean() * 252\n",
        "  year_volatility_true = rets_true.cov() * 252\n",
        "  number_of_assets = m  #real asset number\n",
        "  \n",
        "  def statistics_true(weights):        \n",
        "    weights = np.array(weights)\n",
        "    pret = np.sum(rets_true.mean() * weights) * 252\n",
        "    pvol = np.sqrt(np.dot(weights.T, np.dot(rets_true.cov() * 252, weights)))\n",
        "    return np.array([pret, pvol, (pret-rf) / pvol])\n",
        "  \n",
        "  index=k\n",
        "  return_pre=statistics(opts['x'])[0]\n",
        "  variance_pre=statistics(opts['x'])[1]\n",
        "  sharp_pre=statistics(opts['x'])[2]\n",
        "  return_true=statistics_true(opts['x'])[0]\n",
        "  variance_true=statistics_true(opts['x'])[1]\n",
        "  sharp_true=statistics_true(opts['x'])[2]\n",
        "  \n",
        "  df_all=df_all.append(pd.DataFrame({'index':[index],\n",
        "                                     'return_pre':[return_pre],\n",
        "                                     'variance_pre':[variance_pre],\n",
        "                                     'sharp_pre':[sharp_pre],\n",
        "                                     'return_true':[ return_true],\n",
        "                                     'variance_true':[variance_true],\n",
        "                                    'sharp_true':[sharp_true],}),ignore_index=True)\n",
        "#   print('stock number:',n)\n",
        "#   print('count',k)\n",
        "#   print('choose number',m)\n",
        "#   print('initial random weight',weights)\n",
        "#   print('pre weights',opts['x'])\n",
        "print('Minimum variance portfolio allocation based on LSTM (0-60 days) , all stocks: \\n',df_all)\n",
        "#   print('weight',opts['x'].round(3))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Minimum variance portfolio allocation based on LSTM (0-60 days) , all stocks: \n",
            "   index  return_pre  return_true  sharp_pre  sharp_true  variance_pre  \\\n",
            "0     0    0.741329     0.649279  28.999391    5.148391      0.025046   \n",
            "1     1    0.754430     0.873133  31.026647   12.354335      0.023832   \n",
            "2     2    0.825998     0.470855  15.209476    1.771408      0.053322   \n",
            "3     3    0.719389     0.778536  36.999539    6.016730      0.019038   \n",
            "4     4    0.722221     0.738164  39.553751    7.494261      0.017880   \n",
            "5     5    0.683603    -0.994961  28.452950   -3.269936      0.023499   \n",
            "\n",
            "   variance_true  \n",
            "0       0.123199  \n",
            "1       0.069460  \n",
            "2       0.257340  \n",
            "3       0.126902  \n",
            "4       0.096496  \n",
            "5       0.308863  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aT1fd61eLj_1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Mean variance portfolio allocation based on LSTM (0-60 days) , asset 2."
      ]
    },
    {
      "metadata": {
        "id": "pZT49g-fLoY4",
        "colab_type": "code",
        "outputId": "c250227c-ac36-4e33-9e7d-bb369466bf95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "cell_type": "code",
      "source": [
        "filename=np.load('filename_delete_sort.npy')\n",
        "sort=np.load('new_sortname.npy')\n",
        "\n",
        "LH_dph=np.load('L1_r.npy')\n",
        "LH_dph=pd.DataFrame(LH_dph)\n",
        "LH_dph.columns = ['MSE','TRUE','accuracy','index','predict','stock']\n",
        "LH_dph_ori=LH_dph\n",
        "\n",
        "top=50\n",
        "\n",
        "LH_dph=[]\n",
        "for i in range(0,451):\n",
        "  for j in range(0,top):\n",
        "    if LH_dph_ori['stock'][i]==sort[j]:\n",
        "      LH_dph.append(LH_dph_ori.iloc[i,])\n",
        "      \n",
        "LH_dph=np.array(LH_dph)\n",
        "LH_dph=pd.DataFrame(LH_dph)\n",
        "LH_dph.columns = ['MSE','TRUE','accuracy','index','predict','stock']\n",
        "\n",
        "test_win=6\n",
        "pre_len1=60\n",
        "pre_len2=60\n",
        "rf=0.015\n",
        "stock_len=top\n",
        "\n",
        "df_all=pd.DataFrame(columns=('index','return_pre','variance_pre','sharp_pre',\n",
        "                            'return_true','variance_true','sharp_true',))\n",
        "\n",
        "filename=np.load('filename_delete_sort.npy')\n",
        "df = pd.DataFrame()\n",
        "\n",
        "rets_true_all=pd.DataFrame()\n",
        "####cumulative\n",
        "for k in range(0,test_win):\n",
        "  n=stock_len\n",
        "  \n",
        "  for i in range(0,n):\n",
        "    t=LH_dph['predict'][i][k*pre_len1:k*pre_len1+pre_len1]\n",
        "    t1=[]\n",
        "    for j in range(0,pre_len1):\n",
        "      t1.append(t[j])\n",
        "    t=t1\n",
        "    df[filename[i]]=t  \n",
        "  data1=df\n",
        "  log_returns = np.log(data1 / data1.shift(1))\n",
        "  ret_index = (1 + log_returns).cumprod()\n",
        "  \n",
        "  choose_name=[]\n",
        "  choose_index=[]\n",
        "  for i in range(0,n):\n",
        "    if ret_index[filename[i]][59]>1.03:\n",
        "        choose_name.append(filename[i])\n",
        "        choose_index.append(i)\n",
        "\n",
        "  #choose data\n",
        "  m=len(choose_index)\n",
        "  data2=data1.iloc[:,choose_index]\n",
        "  log_returns = np.log(data2 / data2.shift(1))\n",
        "  \n",
        "  rets = log_returns\n",
        "  number_of_assets = m\n",
        "  weights = np.random.random(number_of_assets)\n",
        "  weights /= np.sum(weights)\n",
        "  \n",
        "  def statistics(weights):        \n",
        "    weights = np.array(weights)\n",
        "    pret = np.sum(rets.mean() * weights) * 252\n",
        "    pvol = np.sqrt(np.dot(weights.T, np.dot(rets.cov() * 252, weights)))\n",
        "    return np.array([pret, pvol, (pret-rf) / pvol])\n",
        "  \n",
        "  def min_func_sharpe(weights):\n",
        "    return -statistics(weights)[2]\n",
        "  \n",
        "  bnds = tuple((0,1) for x in range(number_of_assets))\n",
        "  cons = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
        "\n",
        "  opts = sco.minimize(min_func_sharpe, \n",
        "                      number_of_assets * [1. / number_of_assets,],\n",
        "                      method='SLSQP', \n",
        "                      bounds=bnds, \n",
        "                      constraints=cons)\n",
        "  \n",
        "  weights_pre=opts['x']\n",
        "  \n",
        "  ##check return\n",
        "  df2 = pd.DataFrame()\n",
        "  for i in choose_index:\n",
        "    t=LH_dph['TRUE'][i][k*pre_len1:k*pre_len1+pre_len1]\n",
        "    t1=[]\n",
        "    for j in range(0,pre_len1):\n",
        "      t1.append(t[j])\n",
        "    t= t1\n",
        "    df2['ture'+filename[i]]=t\n",
        "  data3=df2 \n",
        "  log_returns_true = np.log(data3 / data3.shift(1))\n",
        "  \n",
        "  rets_true = log_returns_true\n",
        "  a=rets_true_all.append(rets_true,ignore_index=True) \n",
        "  number_of_assets = m  #real asset number\n",
        "  \n",
        "  def statistics_true(weights):        \n",
        "    weights = np.array(weights)\n",
        "    pret = np.sum(rets_true.mean() * weights) * 252\n",
        "    pvol = np.sqrt(np.dot(weights.T, np.dot(rets_true.cov() * 252, weights)))\n",
        "    return np.array([pret, pvol, (pret-rf) / pvol])\n",
        "  \n",
        "  index=k\n",
        "  return_pre=statistics(opts['x'])[0]\n",
        "  variance_pre=statistics(opts['x'])[1]\n",
        "  sharp_pre=statistics(opts['x'])[2]\n",
        "  return_true=statistics_true(opts['x'])[0]\n",
        "  variance_true=statistics_true(opts['x'])[1]\n",
        "  sharp_true=statistics_true(opts['x'])[2]\n",
        "  \n",
        "  df_all=df_all.append(pd.DataFrame({'index':[index],\n",
        "                                     'return_pre':[return_pre],\n",
        "                                     'variance_pre':[variance_pre],\n",
        "                                     'sharp_pre':[sharp_pre],\n",
        "                                     'return_true':[ return_true],\n",
        "                                     'variance_true':[variance_true],\n",
        "                                    'sharp_true':[sharp_true],}),ignore_index=True)\n",
        "#   print('stock number:',n)\n",
        "#   print('count',k)\n",
        "#   print('choose number',m)\n",
        "#   print('initial random weight',weights)\n",
        "#   print('pre weights',opts['x'])\n",
        "#   print('weight',opts['x'].round(3))\n",
        "print('Mean variance portfolio allocation based on LSTM (0-60 days) , asset 2: \\n',df_all)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean variance portfolio allocation based on LSTM (0-60 days) , asset 2: \n",
            "   index  return_pre  return_true  sharp_pre  sharp_true  variance_pre  \\\n",
            "0     0    0.224812     0.249507  33.925964    3.506234      0.006184   \n",
            "1     1    0.301431     0.279783  22.414117    4.066021      0.012779   \n",
            "2     2    0.237534     0.048788  16.227514    0.177826      0.013713   \n",
            "3     3    0.334172     0.316791  23.052699    2.285801      0.013845   \n",
            "4     4    0.285486     0.299275  38.143200    3.800433      0.007091   \n",
            "5     5    0.153386     0.264725   7.027692    1.518309      0.019692   \n",
            "\n",
            "   variance_true  \n",
            "0       0.066883  \n",
            "1       0.065121  \n",
            "2       0.190005  \n",
            "3       0.132028  \n",
            "4       0.074801  \n",
            "5       0.164476  \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}