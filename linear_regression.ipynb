{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linear regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "QQ59OKphTfGV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Linear regression main files\n",
        "Code editor: Xinyi Li, Yinchuan Li. Date: 2019.2.20.\n",
        "\n",
        "The code is run on Google's Colaboratory with Python 3.\n",
        "\n",
        "Paper: Mid-LSTM meets Mid-ARMA: deep learning for midterm stock prediction."
      ]
    },
    {
      "metadata": {
        "id": "FDeDeAI0Cuwm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Import packages"
      ]
    },
    {
      "metadata": {
        "id": "PXkY2jc4e--4",
        "colab_type": "code",
        "outputId": "1a1ff293-86de-493f-92c9-44066e1e430d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "\n",
        "import os\n",
        "os.chdir(\"drive/download data/sp500new\")\n",
        "# !ls\n",
        "from google.colab import files\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "from numpy import newaxis\n",
        "from keras.layers import Dense, Activation, Dropout, LSTM\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "from math import pi,sqrt,exp,pow,log\n",
        "from numpy.linalg import det, inv\n",
        "from abc import ABCMeta, abstractmethod\n",
        "from sklearn import cluster\n",
        "\n",
        "import statsmodels.api as sm \n",
        "import scipy.stats as scs\n",
        "import scipy.optimize as sco\n",
        "import scipy.interpolate as sci\n",
        "from scipy import stats"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 131322 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.1-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
            "  from pandas.core import datetools\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "HTndDzDKfMgj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def stock_linear_loop (filename):\n",
        "  split = (0.85);\n",
        "  sequence_length=60;\n",
        "  normalise= True\n",
        "  batch_size=60;\n",
        "  input_dim=4\n",
        "  input_timesteps=sequence_length-1\n",
        "  neurons=60\n",
        "  epochs=2\n",
        "  prediction_len=60\n",
        "  dense_output=4\n",
        "  window_size=sequence_length\n",
        "\n",
        "  dataframe = pd.read_csv(filename)\n",
        "\n",
        "  #pre stock put on first column\n",
        "  cols = ['Close_y', 'Volume_y','Close_x'];#corr will be add on later\n",
        "\n",
        "\n",
        "  len_dataframe=dataframe.shape[0]\n",
        "  corr_num=int(len_dataframe/sequence_length)\n",
        "  remainder=len_dataframe-corr_num*sequence_length\n",
        "\n",
        "  # caculate corr table\n",
        "  corr_win=[]\n",
        "  corr=np.zeros((len_dataframe))\n",
        "  for i in range(0,corr_num):\n",
        "    stock1=[]\n",
        "    stock2=[]\n",
        "    for j in range(i*sequence_length,i*sequence_length+sequence_length):\n",
        "      stock1.append(dataframe[cols[0]][j])\n",
        "      stock2.append(dataframe[cols[2]][j])\n",
        "    corr_win.append(np.corrcoef(stock1, stock2)[0,1])\n",
        "    for j in range(i*sequence_length,i*sequence_length+sequence_length):\n",
        "      corr[j]=corr_win[i]\n",
        "\n",
        "\n",
        "  corr_win_remainder=[] \n",
        "  stock1_remainder=[]\n",
        "  stock2_remainder=[] \n",
        "  for k in range(0,remainder):\n",
        "    stock1_remainder.append(dataframe[cols[0]][corr_num*sequence_length+k])\n",
        "    stock2_remainder.append(dataframe[cols[2]][corr_num*sequence_length+k])\n",
        "  corr_win_remainder.append(np.corrcoef(stock1_remainder, stock2_remainder)[0,1])\n",
        "  for q in range(0,remainder):\n",
        "    corr[corr_num*sequence_length+q]=corr_win_remainder[0]\n",
        "\n",
        "  i_split = int(len(dataframe) * split)\n",
        "  data_train = dataframe.get(cols).values[:i_split]\n",
        "  data_test  = dataframe.get(cols).values[i_split:]\n",
        "  len_train  = len(data_train)\n",
        "  len_test   = len(data_test)\n",
        "  len_train_windows = None\n",
        "\n",
        "  corr_df=pd.DataFrame(corr)\n",
        "  data_corr_train=corr_df.values[:i_split]\n",
        "  data_corr_test=corr_df.values[i_split:]\n",
        "\n",
        "  #get_test_data ###############################################################\n",
        "\n",
        "  data_windows = []\n",
        "  for i in range(len_test - sequence_length):\n",
        "    data_windows.append(data_test[i:i+sequence_length])\n",
        "  data_windows = np.array(data_windows).astype(float)\n",
        "\n",
        "  # get original y_test\n",
        "  y_test_ori = data_windows[:, -1, [0]]\n",
        "\n",
        "  window_data=data_windows\n",
        "  win_num=window_data.shape[0]\n",
        "  row_num=window_data.shape[1]\n",
        "  col_num=window_data.shape[2]\n",
        "  normalised_data = []\n",
        "  record_min=[]\n",
        "  record_max=[]\n",
        "\n",
        "  for win_i in range(0,win_num):\n",
        "    normalised_window = []\n",
        "    for col_i in range(0,col_num):\n",
        "      temp_col=window_data[win_i,:,col_i]\n",
        "      temp_min=min(temp_col)\n",
        "      if col_i==0:\n",
        "        record_min.append(temp_min)#record min\n",
        "      temp_col=temp_col-temp_min\n",
        "      temp_max=max(temp_col)\n",
        "      if col_i==0:\n",
        "        record_max.append(temp_max)#record max\n",
        "      temp_col=temp_col/temp_max\n",
        "      normalised_window.append(temp_col)\n",
        "    normalised_window = np.array(normalised_window).T\n",
        "    normalised_data.append(normalised_window)\n",
        "  normalised_data=np.array(normalised_data)\n",
        "\n",
        "  corr_windows = []\n",
        "  for i in range(len_test - sequence_length):\n",
        "    corr_windows.append(data_corr_test[i:i+sequence_length])\n",
        "  corr_windows = np.array(corr_windows).astype(float)\n",
        "\n",
        "  get_test_data=[]\n",
        "  for win_i in range(0,win_num):\n",
        "    df1=pd.DataFrame(normalised_data[win_i,:,:])\n",
        "    df1['corr']=corr_windows[win_i,:,:]\n",
        "    df2=df1.values\n",
        "    get_test_data.append(df2)\n",
        "  get_test_data=np.array(get_test_data)  \n",
        "\n",
        "  data_windows=get_test_data\n",
        "  x_test = data_windows[:, :-1]\n",
        "  y_test = data_windows[:, -1, [0]]\n",
        "\n",
        "  #get_train_data ##############################################################\n",
        "  data_windows = []\n",
        "  for i in range(len_train - sequence_length):\n",
        "    data_windows.append(data_train[i:i+sequence_length])\n",
        "  data_windows = np.array(data_windows).astype(float)\n",
        "\n",
        "  window_data=data_windows\n",
        "  win_num=window_data.shape[0]\n",
        "  row_num=window_data.shape[1]\n",
        "  col_num=window_data.shape[2]\n",
        "\n",
        "  normalised_data = []\n",
        "  for win_i in range(0,win_num):\n",
        "    normalised_window = []\n",
        "    for col_i in range(0,col_num):\n",
        "      temp_col=window_data[win_i,:,col_i]\n",
        "      temp_min=min(temp_col)\n",
        "      temp_col=temp_col-temp_min\n",
        "      temp_max=max(temp_col)\n",
        "      temp_col=temp_col/temp_max\n",
        "      normalised_window.append(temp_col)\n",
        "    normalised_window = np.array(normalised_window).T\n",
        "    normalised_data.append(normalised_window)\n",
        "  normalised_data=np.array(normalised_data)\n",
        "\n",
        "  corr_windows_train = []\n",
        "  for i in range(len_train - sequence_length):\n",
        "    corr_windows_train.append(data_corr_train[i:i+sequence_length])\n",
        "  corr_windows_train = np.array(corr_windows_train).astype(float)\n",
        "\n",
        "  get_train_data=[]\n",
        "  for win_i in range(0,win_num):\n",
        "    df1=pd.DataFrame(normalised_data[win_i,:,:])\n",
        "    df1['corr']=corr_windows_train[win_i,:,:]\n",
        "    df2=df1.values\n",
        "    get_train_data.append(df2)\n",
        "  get_train_data=np.array(get_train_data)  \n",
        "\n",
        "  data_windows=get_train_data\n",
        "  x_train = data_windows[:, :-1]\n",
        "  y_train = data_windows[:, -1]\n",
        "\n",
        "  ## Linear regression\n",
        "  x_train_l=pd.DataFrame(x_train[:,:,0])\n",
        "  y_train_l=pd.DataFrame(y_train[:,0])\n",
        "  x_test_l=pd.DataFrame(x_test[:,:,0])\n",
        "  y_test_l=pd.DataFrame(y_test[:,0])\n",
        "  y_test_ori_l=pd.DataFrame(y_test_ori[:,0])\n",
        "\n",
        "\n",
        "  model = LinearRegression(fit_intercept=True)\n",
        "  model.fit(x_train_l, y_train_l)\n",
        "  # print(\"Model slope:    \", model.coef_[0])\n",
        "  # print(\"Model intercept:\", model.intercept_)\n",
        "\n",
        "  data=x_test[:,:,0]\n",
        "  prediction_seqs = []\n",
        "  pre_win_num=int(len(data)/prediction_len)\n",
        "\n",
        "  for i in range(0,pre_win_num):\n",
        "    curr_frame = data[i*prediction_len]\n",
        "    predicted = []\n",
        "    for j in range(0,prediction_len):\n",
        "      predicted.append(model.predict(curr_frame[newaxis,:])[0])\n",
        "      curr_frame = curr_frame[1:]\n",
        "      curr_frame = np.insert(curr_frame, [window_size-2], predicted[-1], axis=0)\n",
        "    prediction_seqs.append(predicted)\n",
        "\n",
        "  #de_predicted\n",
        "  de_predicted=[]\n",
        "  len_pre_win=int(len(data)/prediction_len)\n",
        "  len_pre=prediction_len\n",
        "\n",
        "  m=0\n",
        "  for i in range(0,len_pre_win):\n",
        "    for j in range(0,len_pre):\n",
        "      de_predicted.append(prediction_seqs[i][j][0]*record_max[m]+record_min[m])\n",
        "      m=m+1\n",
        "      \n",
        "  error = []\n",
        "  diff=y_test.shape[0]-prediction_len*pre_win_num\n",
        "\n",
        "  for i in range(y_test_ori.shape[0]-diff):\n",
        "      error.append(y_test_ori[i,] - de_predicted[i])\n",
        "\n",
        "  squaredError = []\n",
        "  absError = []\n",
        "  for val in error:\n",
        "      squaredError.append(val * val) \n",
        "      absError.append(abs(val))\n",
        "\n",
        "  error_percent=[]\n",
        "  for i in range(len(error)):\n",
        "    val=absError[i]/y_test_ori[i,]\n",
        "    val=abs(val)\n",
        "    error_percent.append(val)\n",
        "\n",
        "  mean_error_percent=np.mean(error_percent)\n",
        "  accuracy=1-mean_error_percent\n",
        "  MSE=sum(squaredError) / len(squaredError)\n",
        "  return MSE,accuracy,y_test_ori,de_predicted\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ADF5BQLYfP4u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filename=np.load('filename_delete_sort.npy')\n",
        "\n",
        "result_linear_df=pd.DataFrame(columns=('index','stock','MSE','accuracy','true','predict'))\n",
        "n=len(filename)\n",
        "                           \n",
        "for i in range(0,n):\n",
        "  index=i\n",
        "  stock=filename[i]\n",
        "  result=stock_linear_loop(filename[i])\n",
        "#   MSE=stock_loop(filename_list[i])\n",
        "  MSE=result[0]\n",
        "  accuracy=result[1]\n",
        "  true=result[2]\n",
        "  predict=result[3]\n",
        "  result_linear_df=result_linear_df.append(pd.DataFrame({'index':[index],\n",
        "                                                     'stock':[stock],\n",
        "                                                     'MSE':[MSE],\n",
        "                                                     'accuracy':[accuracy],\n",
        "                                                     'true':[true],\n",
        "                                                     'predict':[predict]}),ignore_index=True)\n",
        "  print(i)\n",
        "  np.save('linear_d0.npy',result_linear_df)\n",
        "  result_linear_df.to_csv('linear_d0.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S9ivaEp8fk9g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Reshape data\n",
        "L4_all=np.load('linear_d0.npy')\n",
        "L4_all=pd.DataFrame(L4_all)\n",
        "L4_all.columns=['MSE','accuracy','index','predict','stock','TRUE']\n",
        "\n",
        "filename=np.load('filename_delete_sort.npy')\n",
        "n=len(filename)\n",
        "len_pre=360\n",
        "\n",
        "result_df=pd.DataFrame(columns=('index','stock','TRUE','predict','accuracy','MSE'))\n",
        "for i in range(0,n):\n",
        "  index=i\n",
        "  stock=filename[i]\n",
        "  #TRUE\n",
        "  t=[]\n",
        "  for j in range(0,len_pre):\n",
        "    t.append(L4_all['TRUE'][i][j][0])\n",
        "  TRUE=t\n",
        "  #predict\n",
        "  t=[]\n",
        "  for j in range(0,len_pre):\n",
        "    t.append(L4_all['predict'][i][j])\n",
        "  predict=t\n",
        "  #accuracy\n",
        "  accuracy=[]\n",
        "  for j in range(0,len_pre):\n",
        "    t=abs(TRUE[j]-predict[j])/TRUE[j]\n",
        "    t1=1-t\n",
        "    accuracy.append(t1)\n",
        "  accuracy=accuracy\n",
        "  #MSE\n",
        "  MSE=L4_all['MSE'][i][0]\n",
        "  result_df=result_df.append(pd.DataFrame({'index':index,'stock':[stock],\n",
        "                                           'TRUE':[TRUE],\n",
        "                                           'predict':[predict],\n",
        "                                          'accuracy':[accuracy],\n",
        "                                          'MSE':[MSE]}),\n",
        "                             ignore_index=True)\n",
        "  print(i)\n",
        "np.save('linear_r.npy',result_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ay_PKL3WhCX4",
        "colab_type": "code",
        "outputId": "b8bdc347-a00b-4918-d8e8-7b5086c7547e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "##Mean MPA of all stocks\n",
        "linear_r=np.load('linear_r.npy')\n",
        "linear_r=pd.DataFrame(linear_r)\n",
        "linear_r.columns=['MSE','TRUE','accuracy','index','predict','stock']\n",
        "\n",
        "n=451\n",
        "avg_accuracy1=[]\n",
        "for i in range(0,360):\n",
        "  t1=0\n",
        "  for j in range(0,n):\n",
        "    t1=t1+linear_r['accuracy'][j][i]\n",
        "  t1=t1/n\n",
        "  avg_accuracy1.append(t1)\n",
        "\n",
        "half1=[]\n",
        "for i in range(0,6): \n",
        "  half1.extend(avg_accuracy1[60*(i+1)-30:60*(i+1)]) \n",
        "mean1=pd.DataFrame(half1).mean()[0]\n",
        "\n",
        "print('Linear regression Mean MPA: ',mean1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear regression Mean MPA:  0.9252737179348499\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uOV4QdhWFj9Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Portfolio allocation"
      ]
    },
    {
      "metadata": {
        "id": "Fn2XmyuOFn6f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Mean variance portfolio allocation based on linear regression (0-60 days) , asset 1."
      ]
    },
    {
      "metadata": {
        "id": "52be00gCFnZt",
        "colab_type": "code",
        "outputId": "eb9f75bc-47f3-4d1a-c206-2d5c2a1b6afd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "cell_type": "code",
      "source": [
        "LH_dph=np.load('linear_r.npy')\n",
        "LH_dph=pd.DataFrame(LH_dph)\n",
        "LH_dph.columns = ['MSE','TRUE','accuracy','index','predict','stock']\n",
        "\n",
        "test_win=6\n",
        "pre_len1=60\n",
        "pre_len2=60\n",
        "stock_len=451\n",
        "rf=0.015#risk free rate\n",
        "\n",
        "df_all=pd.DataFrame(columns=('index','return_pre','variance_pre','sharp_pre',\n",
        "                            'return_true','variance_true','sharp_true',))\n",
        "\n",
        "filename=np.load('filename_delete_sort.npy')\n",
        "df = pd.DataFrame()\n",
        "\n",
        "####cumulative\n",
        "for k in range(0,test_win):\n",
        "  n=stock_len\n",
        "  \n",
        "  for i in range(0,n):\n",
        "    t=LH_dph['predict'][i][k*pre_len1:k*pre_len1+pre_len1]\n",
        "    t1=[]\n",
        "    for j in range(0,pre_len1):\n",
        "      t1.append(t[j])\n",
        "    t=t1\n",
        "    df[filename[i]]=t  \n",
        "  data1=df\n",
        "  log_returns = np.log(data1 / data1.shift(1))\n",
        "  ret_index = (1 + log_returns).cumprod()\n",
        "  \n",
        "  choose_name=[]\n",
        "  choose_index=[]\n",
        "  for i in range(0,n):\n",
        "    if ret_index[filename[i]][59]>1.15:\n",
        "      choose_name.append(filename[i])\n",
        "      choose_index.append(i)\n",
        "      \n",
        "  #choose data\n",
        "  m=len(choose_index)\n",
        "  data2=data1.iloc[:,choose_index]\n",
        "  log_returns = np.log(data2 / data2.shift(1))\n",
        "  \n",
        "  rets = log_returns\n",
        "  year_ret = rets.mean() * 252\n",
        "  year_volatility = rets.cov() * 252\n",
        "  number_of_assets = m\n",
        "  weights = np.random.random(number_of_assets)\n",
        "  weights /= np.sum(weights)\n",
        "  \n",
        "  def statistics(weights):        \n",
        "    weights = np.array(weights)\n",
        "    pret = np.sum(rets.mean() * weights) * 252\n",
        "    pvol = np.sqrt(np.dot(weights.T, np.dot(rets.cov() * 252, weights)))\n",
        "    return np.array([pret, pvol, (pret-rf) / pvol])\n",
        "  \n",
        "  def min_func_sharpe(weights):\n",
        "    return -statistics(weights)[2]\n",
        "    \n",
        "  bnds = tuple((0,1) for x in range(number_of_assets))\n",
        "  cons = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
        "\n",
        "  opts = sco.minimize(min_func_sharpe, \n",
        "                      number_of_assets * [1. / number_of_assets,],\n",
        "                      method='SLSQP', \n",
        "                      bounds=bnds, \n",
        "                      constraints=cons)\n",
        "  \n",
        "  weights_pre=opts['x']\n",
        "  \n",
        "  ##check return\n",
        "  df2 = pd.DataFrame()\n",
        "  for i in choose_index:\n",
        "    t=LH_dph['TRUE'][i][k*pre_len1:k*pre_len1+pre_len1]\n",
        "    t1=[]\n",
        "    for j in range(0,pre_len1):\n",
        "      t1.append(t[j])\n",
        "    t= t1\n",
        "    df2['ture'+filename[i]]=t\n",
        "  data3=df2 \n",
        "  log_returns_true = np.log(data3 / data3.shift(1))\n",
        "  \n",
        "  rets_true = log_returns_true\n",
        "  year_ret_true = rets_true.mean() * 252\n",
        "  year_volatility_true = rets_true.cov() * 252\n",
        "  number_of_assets = m  #real asset number\n",
        "  \n",
        "  def statistics_true(weights):        \n",
        "    weights = np.array(weights)\n",
        "    pret = np.sum(rets_true.mean() * weights) * 252\n",
        "    pvol = np.sqrt(np.dot(weights.T, np.dot(rets_true.cov() * 252, weights)))\n",
        "    return np.array([pret, pvol, (pret-rf) / pvol])\n",
        "  \n",
        "  index=k\n",
        "  return_pre=statistics(opts['x'])[0]\n",
        "  variance_pre=statistics(opts['x'])[1]\n",
        "  sharp_pre=statistics(opts['x'])[2]\n",
        "  return_true=statistics_true(opts['x'])[0]\n",
        "  variance_true=statistics_true(opts['x'])[1]\n",
        "  sharp_true=statistics_true(opts['x'])[2]\n",
        "  \n",
        "  df_all=df_all.append(pd.DataFrame({'index':[index],\n",
        "                                     'return_pre':[return_pre],\n",
        "                                     'variance_pre':[variance_pre],\n",
        "                                     'sharp_pre':[sharp_pre],\n",
        "                                     'return_true':[ return_true],\n",
        "                                     'variance_true':[variance_true],\n",
        "                                    'sharp_true':[sharp_true],}),ignore_index=True)\n",
        "#   print('stock number:',n)\n",
        "#   print('count',k)\n",
        "#   print('choose number',m)\n",
        "#   print('initial random weight',weights)\n",
        "#   print('pre weights',opts['x'])\n",
        "print('Mean variance portfolio allocation based on linear regression (0-60 days) , all stocks: \\n',df_all)\n",
        "#   print('weight',opts['x'].round(3))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean variance portfolio allocation based on linear regression (0-60 days) , all stocks: \n",
            "   index  return_pre  return_true  sharp_pre  sharp_true  variance_pre  \\\n",
            "0     0    0.796731     0.780742  16.851472    4.653189      0.046389   \n",
            "1     1    0.920995     1.221869  26.328534   10.292658      0.034411   \n",
            "2     2    0.748953     0.573060   8.939460    2.163487      0.082103   \n",
            "3     3    0.793091     0.704286  44.678987    4.922037      0.017415   \n",
            "4     4    0.765760     0.809170  42.677197    7.884520      0.017592   \n",
            "5     5    0.779626     0.734158  12.798729    3.240103      0.059742   \n",
            "\n",
            "   variance_true  \n",
            "0       0.164563  \n",
            "1       0.117255  \n",
            "2       0.257945  \n",
            "3       0.140041  \n",
            "4       0.100725  \n",
            "5       0.221955  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E7RZQKW9IPWa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Minimum variance portfolio allocation based on linear regression (0-60 days) , asset 1."
      ]
    },
    {
      "metadata": {
        "id": "HtUdndCTITGE",
        "colab_type": "code",
        "outputId": "39b4d672-1c8f-425f-c439-ec4fc9646b71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "cell_type": "code",
      "source": [
        "LH_dph=np.load('linear_r.npy')\n",
        "LH_dph=pd.DataFrame(LH_dph)\n",
        "LH_dph.columns = ['MSE','TRUE','accuracy','index','predict','stock']\n",
        "\n",
        "test_win=6\n",
        "pre_len1=60\n",
        "pre_len2=60\n",
        "stock_len=451\n",
        "rf=0.015#risk free rate\n",
        "\n",
        "df_all=pd.DataFrame(columns=('index','return_pre','variance_pre','sharp_pre',\n",
        "                            'return_true','variance_true','sharp_true',))\n",
        "\n",
        "filename=np.load('filename_delete_sort.npy')\n",
        "df = pd.DataFrame()\n",
        "\n",
        "####cumulative\n",
        "for k in range(0,test_win):\n",
        "  n=stock_len\n",
        "  \n",
        "  for i in range(0,n):\n",
        "    t=LH_dph['predict'][i][k*pre_len1:k*pre_len1+pre_len1]\n",
        "    t1=[]\n",
        "    for j in range(0,pre_len1):\n",
        "      t1.append(t[j])\n",
        "    t=t1\n",
        "    df[filename[i]]=t  \n",
        "  data1=df\n",
        "  log_returns = np.log(data1 / data1.shift(1))\n",
        "  ret_index = (1 + log_returns).cumprod()\n",
        "  \n",
        "  choose_name=[]\n",
        "  choose_index=[]\n",
        "  for i in range(0,n):\n",
        "    if ret_index[filename[i]][59]>1.15:\n",
        "      choose_name.append(filename[i])\n",
        "      choose_index.append(i)\n",
        "      \n",
        "  #choose data\n",
        "  m=len(choose_index)\n",
        "  data2=data1.iloc[:,choose_index]\n",
        "  log_returns = np.log(data2 / data2.shift(1))\n",
        "  \n",
        "  rets = log_returns\n",
        "  year_ret = rets.mean() * 252\n",
        "  year_volatility = rets.cov() * 252\n",
        "  number_of_assets = m\n",
        "  weights = np.random.random(number_of_assets)\n",
        "  weights /= np.sum(weights)\n",
        "  \n",
        "  def statistics(weights):        \n",
        "    weights = np.array(weights)\n",
        "    pret = np.sum(rets.mean() * weights) * 252\n",
        "    pvol = np.sqrt(np.dot(weights.T, np.dot(rets.cov() * 252, weights)))\n",
        "    return np.array([pret, pvol, (pret-rf) / pvol])\n",
        "  \n",
        "  def min_func_sharpe(weights):\n",
        "    return statistics(weights)[1]\n",
        "    \n",
        "  bnds = tuple((0,1) for x in range(number_of_assets))\n",
        "  cons = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
        "\n",
        "  opts = sco.minimize(min_func_sharpe, \n",
        "                      number_of_assets * [1. / number_of_assets,],\n",
        "                      method='SLSQP', \n",
        "                      bounds=bnds, \n",
        "                      constraints=cons)\n",
        "  \n",
        "  weights_pre=opts['x']\n",
        "  \n",
        "  ##check return\n",
        "  df2 = pd.DataFrame()\n",
        "  for i in choose_index:\n",
        "    t=LH_dph['TRUE'][i][k*pre_len1:k*pre_len1+pre_len1]\n",
        "    t1=[]\n",
        "    for j in range(0,pre_len1):\n",
        "      t1.append(t[j])\n",
        "    t= t1\n",
        "    df2['ture'+filename[i]]=t\n",
        "  data3=df2 \n",
        "  log_returns_true = np.log(data3 / data3.shift(1))\n",
        "  \n",
        "  rets_true = log_returns_true\n",
        "  year_ret_true = rets_true.mean() * 252\n",
        "  year_volatility_true = rets_true.cov() * 252\n",
        "  number_of_assets = m  #real asset number\n",
        "  \n",
        "  def statistics_true(weights):        \n",
        "    weights = np.array(weights)\n",
        "    pret = np.sum(rets_true.mean() * weights) * 252\n",
        "    pvol = np.sqrt(np.dot(weights.T, np.dot(rets_true.cov() * 252, weights)))\n",
        "    return np.array([pret, pvol, (pret-rf) / pvol])\n",
        "  \n",
        "  index=k\n",
        "  return_pre=statistics(opts['x'])[0]\n",
        "  variance_pre=statistics(opts['x'])[1]\n",
        "  sharp_pre=statistics(opts['x'])[2]\n",
        "  return_true=statistics_true(opts['x'])[0]\n",
        "  variance_true=statistics_true(opts['x'])[1]\n",
        "  sharp_true=statistics_true(opts['x'])[2]\n",
        "  \n",
        "  df_all=df_all.append(pd.DataFrame({'index':[index],\n",
        "                                     'return_pre':[return_pre],\n",
        "                                     'variance_pre':[variance_pre],\n",
        "                                     'sharp_pre':[sharp_pre],\n",
        "                                     'return_true':[ return_true],\n",
        "                                     'variance_true':[variance_true],\n",
        "                                    'sharp_true':[sharp_true],}),ignore_index=True)\n",
        "#   print('stock number:',n)\n",
        "#   print('count',k)\n",
        "#   print('choose number',m)\n",
        "#   print('initial random weight',weights)\n",
        "#   print('pre weights',opts['x'])\n",
        "print('Minimum variance portfolio allocation based on linear regression (0-60 days) , all stocks: \\n',df_all)\n",
        "#   print('weight',opts['x'].round(3))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Minimum variance portfolio allocation based on linear regression (0-60 days) , all stocks: \n",
            "   index  return_pre  return_true  sharp_pre  sharp_true  variance_pre  \\\n",
            "0     0    0.754770     0.732216  16.407794    4.312029      0.045086   \n",
            "1     1    0.875726     1.159407  25.697910    9.515437      0.033494   \n",
            "2     2    0.711778     0.609006   8.709688    2.358677      0.080000   \n",
            "3     3    0.720204     0.627257  42.462795    4.512381      0.016608   \n",
            "4     4    0.721244     0.770368  41.374939    7.751476      0.017069   \n",
            "5     5    0.751611     0.653848  12.560595    3.030001      0.058645   \n",
            "\n",
            "   variance_true  \n",
            "0       0.166329  \n",
            "1       0.120268  \n",
            "2       0.251839  \n",
            "3       0.135684  \n",
            "4       0.097448  \n",
            "5       0.210841  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wjC_gttyIn88",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Mean variance portfolio allocation based on linear regression (0-60 days) , asset 2."
      ]
    },
    {
      "metadata": {
        "id": "U4OeTLryFi6t",
        "colab_type": "code",
        "outputId": "cb276e3e-b756-4802-b12e-af4f24cbc7ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "cell_type": "code",
      "source": [
        "filename=np.load('filename_delete_sort.npy')\n",
        "sort=np.load('new_sortname.npy')\n",
        "\n",
        "LH_dph=np.load('linear_r.npy')\n",
        "LH_dph=pd.DataFrame(LH_dph)\n",
        "LH_dph.columns = ['MSE','TRUE','accuracy','index','predict','stock']\n",
        "LH_dph_ori=LH_dph\n",
        "\n",
        "top=50\n",
        "\n",
        "LH_dph=[]\n",
        "for i in range(0,451):\n",
        "  for j in range(0,top):\n",
        "    if LH_dph_ori['stock'][i]==sort[j]:\n",
        "      LH_dph.append(LH_dph_ori.iloc[i,])\n",
        "      \n",
        "LH_dph=np.array(LH_dph)\n",
        "LH_dph=pd.DataFrame(LH_dph)\n",
        "LH_dph.columns = ['MSE','TRUE','accuracy','index','predict','stock']\n",
        "\n",
        "test_win=6\n",
        "pre_len1=60\n",
        "pre_len2=60\n",
        "rf=0.015\n",
        "stock_len=top\n",
        "\n",
        "df_all=pd.DataFrame(columns=('index','return_pre','variance_pre','sharp_pre',\n",
        "                            'return_true','variance_true','sharp_true',))\n",
        "\n",
        "filename=np.load('filename_delete_sort.npy')\n",
        "df = pd.DataFrame()\n",
        "\n",
        "rets_true_all=pd.DataFrame()\n",
        "####cumulative\n",
        "for k in range(0,test_win):\n",
        "  n=stock_len\n",
        "  \n",
        "  for i in range(0,n):\n",
        "    t=LH_dph['predict'][i][k*pre_len1:k*pre_len1+pre_len1]\n",
        "    t1=[]\n",
        "    for j in range(0,pre_len1):\n",
        "      t1.append(t[j])\n",
        "    t=t1\n",
        "    df[filename[i]]=t  \n",
        "  data1=df\n",
        "  log_returns = np.log(data1 / data1.shift(1))\n",
        "  ret_index = (1 + log_returns).cumprod()\n",
        "  \n",
        "  choose_name=[]\n",
        "  choose_index=[]\n",
        "  for i in range(0,n):\n",
        "    if ret_index[filename[i]][59]>1.03:\n",
        "        choose_name.append(filename[i])\n",
        "        choose_index.append(i)\n",
        "\n",
        "  #choose data\n",
        "  m=len(choose_index)\n",
        "  data2=data1.iloc[:,choose_index]\n",
        "  log_returns = np.log(data2 / data2.shift(1))\n",
        "  \n",
        "  rets = log_returns\n",
        "  number_of_assets = m\n",
        "  weights = np.random.random(number_of_assets)\n",
        "  weights /= np.sum(weights)\n",
        "  \n",
        "  def statistics(weights):        \n",
        "    weights = np.array(weights)\n",
        "    pret = np.sum(rets.mean() * weights) * 252\n",
        "    pvol = np.sqrt(np.dot(weights.T, np.dot(rets.cov() * 252, weights)))\n",
        "    return np.array([pret, pvol, (pret-rf) / pvol])\n",
        "  \n",
        "  def min_func_sharpe(weights):\n",
        "    return -statistics(weights)[2]\n",
        "  \n",
        "  bnds = tuple((0,1) for x in range(number_of_assets))\n",
        "  cons = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
        "\n",
        "  opts = sco.minimize(min_func_sharpe, \n",
        "                      number_of_assets * [1. / number_of_assets,],\n",
        "                      method='SLSQP', \n",
        "                      bounds=bnds, \n",
        "                      constraints=cons)\n",
        "  \n",
        "  weights_pre=opts['x']\n",
        "  \n",
        "  ##check return\n",
        "  df2 = pd.DataFrame()\n",
        "  for i in choose_index:\n",
        "    t=LH_dph['TRUE'][i][k*pre_len1:k*pre_len1+pre_len1]\n",
        "    t1=[]\n",
        "    for j in range(0,pre_len1):\n",
        "      t1.append(t[j])\n",
        "    t= t1\n",
        "    df2['ture'+filename[i]]=t\n",
        "  data3=df2 \n",
        "  log_returns_true = np.log(data3 / data3.shift(1))\n",
        "  \n",
        "  rets_true = log_returns_true\n",
        "  a=rets_true_all.append(rets_true,ignore_index=True) \n",
        "  number_of_assets = m  #real asset number\n",
        "  \n",
        "  def statistics_true(weights):        \n",
        "    weights = np.array(weights)\n",
        "    pret = np.sum(rets_true.mean() * weights) * 252\n",
        "    pvol = np.sqrt(np.dot(weights.T, np.dot(rets_true.cov() * 252, weights)))\n",
        "    return np.array([pret, pvol, (pret-rf) / pvol])\n",
        "  \n",
        "  index=k\n",
        "  return_pre=statistics(opts['x'])[0]\n",
        "  variance_pre=statistics(opts['x'])[1]\n",
        "  sharp_pre=statistics(opts['x'])[2]\n",
        "  return_true=statistics_true(opts['x'])[0]\n",
        "  variance_true=statistics_true(opts['x'])[1]\n",
        "  sharp_true=statistics_true(opts['x'])[2]\n",
        "  \n",
        "  df_all=df_all.append(pd.DataFrame({'index':[index],\n",
        "                                     'return_pre':[return_pre],\n",
        "                                     'variance_pre':[variance_pre],\n",
        "                                     'sharp_pre':[sharp_pre],\n",
        "                                     'return_true':[ return_true],\n",
        "                                     'variance_true':[variance_true],\n",
        "                                    'sharp_true':[sharp_true],}),ignore_index=True)\n",
        "#   print('stock number:',n)\n",
        "#   print('count',k)\n",
        "#   print('choose number',m)\n",
        "#   print('initial random weight',weights)\n",
        "#   print('pre weights',opts['x'])\n",
        "#   print('weight',opts['x'].round(3))\n",
        "print('Mean variance portfolio allocation based on linear regression (0-60 days) , asset 2: \\n',df_all)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean variance portfolio allocation based on linear regression (0-60 days) , asset 2: \n",
            "   index  return_pre  return_true  sharp_pre  sharp_true  variance_pre  \\\n",
            "0     0    0.220914     0.250987  17.772584    2.693912      0.011586   \n",
            "1     1    0.227959     0.209092  12.806178    2.798024      0.016629   \n",
            "2     2    0.361816     0.254532  10.860803    1.329002      0.031933   \n",
            "3     3    0.441395     0.252330  24.266354    1.962841      0.017571   \n",
            "4     4    0.374220     0.338424  33.315874    3.560895      0.010782   \n",
            "5     5    0.271200     0.146359  14.909892    0.894333      0.017183   \n",
            "\n",
            "   variance_true  \n",
            "0       0.087600  \n",
            "1       0.069367  \n",
            "2       0.180235  \n",
            "3       0.120911  \n",
            "4       0.090827  \n",
            "5       0.146879  \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}